{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"final제출.ipynb","provenance":[],"collapsed_sections":["4JVweWIyEYnn","8dkW4Ra0EzJu","i9u1hNLSOYML","2xz4Z100Oqh3","_KaV9mSwQFk1","vVp_imwLP4sE","KBZX_FTxQlbS","hXOJFIUy-nb0","9_kQSe2O-say","oNfqa98dVg7O","Ulmfa5olQTGT","TXJB6VBhehAN"],"toc_visible":true,"authorship_tag":"ABX9TyP27gFYPzD1B2EdM+x5OCRG"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"4JVweWIyEYnn","colab_type":"text"},"source":["## most_popular 만들기 (첨부한 파일 그대로 사용하면 실행 안해도됩니다!) : 베이스라인코드"]},{"cell_type":"markdown","metadata":{"id":"8dkW4Ra0EzJu","colab_type":"text"},"source":["### 파이참에서 베이스라인코드양식그대로 사용하여 나온 results를 most_pupular로이름을 변경한 후 저장하여 사용했습니다. ( 곡을전혀 뽑지 못하는 플레이리스트에 적용하였습니다.)"]},{"cell_type":"code","metadata":{"id":"KHUUFeiREisy","colab_type":"code","colab":{}},"source":["python genre_most_popular.py run song_meta.json train.json test.json"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i9u1hNLSOYML","colab_type":"text"},"source":["## 토크나이징을 위한 khaiii설치"]},{"cell_type":"code","metadata":{"id":"cQD3UpT7OJp3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1595760442917,"user_tz":-540,"elapsed":245781,"user":{"displayName":"이은열","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-vtFZD8JYZ-d5KFMLPO6e-yNOvrn2o_9v2sV-=s64","userId":"16187002805945562758"}},"outputId":"880174cb-e1aa-461d-b4cd-065b2fafd9f8"},"source":["\n","!git clone https://github.com/kakao/khaiii.git\n","!cd /content/khaiii/ && mkdir build\n","!cd /content/khaiii/build && pip install cmake\n","!cd /content/khaiii/build && cmake /content/khaiii/build\n","!cd /content/khaiii/build && cmake ..\n","!cd /content/khaiii/build/ && make all\n","!cd /content/khaiii/build/ && make resource\n","!cd /content/khaiii/build/ && make install\n","!cd /content/khaiii/build/ && make package_python\n","!cd /content/khaiii/build/package_python && pip install .\n"],"execution_count":33,"outputs":[{"output_type":"stream","text":["fatal: destination path 'khaiii' already exists and is not an empty directory.\n","mkdir: cannot create directory ‘build’: File exists\n","Requirement already satisfied: cmake in /usr/local/lib/python3.6/dist-packages (3.12.0)\n","-- [khaiii] fused multiply add option enabled\n","-- [hunter] Calculating Toolchain-SHA1\n","-- [hunter] Calculating Config-SHA1\n","-- [hunter] HUNTER_ROOT: /root/.hunter\n","-- [hunter] [ Hunter-ID: 70287b1 | Toolchain-ID: 02ccb06 | Config-ID: dffbc08 ]\n","-- [hunter] BOOST_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 1.68.0-p1)\n","-- Boost version: 1.68.0\n","-- [hunter] CXXOPTS_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 2.1.1-pre)\n","-- [hunter] EIGEN_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 3.3.5)\n","-- [hunter] FMT_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 4.1.0)\n","-- [hunter] GTEST_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 1.8.0-hunter-p11)\n","-- [hunter] NLOHMANN_JSON_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 3.3.0)\n","-- [hunter] SPDLOG_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 0.16.3-p1)\n","-- Configuring done\n","-- Generating done\n","-- Build files have been written to: /content/khaiii/build\n","-- [khaiii] fused multiply add option enabled\n","-- [hunter] Calculating Toolchain-SHA1\n","-- [hunter] Calculating Config-SHA1\n","-- [hunter] HUNTER_ROOT: /root/.hunter\n","-- [hunter] [ Hunter-ID: 70287b1 | Toolchain-ID: 02ccb06 | Config-ID: dffbc08 ]\n","-- [hunter] BOOST_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 1.68.0-p1)\n","-- Boost version: 1.68.0\n","-- [hunter] CXXOPTS_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 2.1.1-pre)\n","-- [hunter] EIGEN_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 3.3.5)\n","-- [hunter] FMT_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 4.1.0)\n","-- [hunter] GTEST_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 1.8.0-hunter-p11)\n","-- [hunter] NLOHMANN_JSON_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 3.3.0)\n","-- [hunter] SPDLOG_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 0.16.3-p1)\n","-- Configuring done\n","-- Generating done\n","-- Build files have been written to: /content/khaiii/build\n","[ 65%] Built target obj_khaiii\n","[ 69%] Built target khaiii\n","[ 76%] Built target bin_khaiii\n","[100%] Built target test_khaiii\n","Built target resource\n","[ 65%] Built target obj_khaiii\n","[ 69%] Built target khaiii\n","[ 76%] Built target bin_khaiii\n","[100%] Built target test_khaiii\n","\u001b[36mInstall the project...\u001b[0m\n","-- Install configuration: \"\"\n","-- Up-to-date: /usr/local/include/khaiii\n","-- Up-to-date: /usr/local/include/khaiii/khaiii_api.h\n","-- Up-to-date: /usr/local/include/khaiii/KhaiiiApi.hpp\n","-- Up-to-date: /usr/local/include/khaiii/khaiii_dev.h\n","-- Up-to-date: /usr/local/share/khaiii\n","-- Up-to-date: /usr/local/share/khaiii/hdn2tag.lin\n","-- Up-to-date: /usr/local/share/khaiii/errpatch.val\n","-- Up-to-date: /usr/local/share/khaiii/cnv2hdn.lin\n","-- Up-to-date: /usr/local/share/khaiii/config.json\n","-- Up-to-date: /usr/local/share/khaiii/restore.val\n","-- Up-to-date: /usr/local/share/khaiii/restore.one\n","-- Up-to-date: /usr/local/share/khaiii/errpatch.len\n","-- Up-to-date: /usr/local/share/khaiii/embed.bin\n","-- Up-to-date: /usr/local/share/khaiii/conv.3.fil\n","-- Up-to-date: /usr/local/share/khaiii/conv.2.fil\n","-- Up-to-date: /usr/local/share/khaiii/conv.4.fil\n","-- Up-to-date: /usr/local/share/khaiii/preanal.val\n","-- Up-to-date: /usr/local/share/khaiii/restore.key\n","-- Up-to-date: /usr/local/share/khaiii/conv.5.fil\n","-- Up-to-date: /usr/local/share/khaiii/errpatch.tri\n","-- Up-to-date: /usr/local/share/khaiii/preanal.tri\n","-- Up-to-date: /usr/local/lib/libkhaiii.so.0.4\n","-- Up-to-date: /usr/local/lib/libkhaiii.so.0\n","-- Up-to-date: /usr/local/lib/libkhaiii.so\n","-- Up-to-date: /usr/local/bin/khaiii\n","\u001b[36mRun CPack packaging tool for source...\u001b[0m\n","CPack: Create package using ZIP\n","CPack: Install projects\n","CPack: - Install directory: /content/khaiii\n","CPack: Create package\n","CPack: - package: /content/khaiii/build/khaiii-0.4.zip generated.\n","Built target package_python\n","Processing /content/khaiii/build/package_python\n","Building wheels for collected packages: khaiii\n","  Building wheel for khaiii (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for khaiii: filename=khaiii-0.4-cp36-none-any.whl size=22883499 sha256=1b892284bd3cd689eca26e58bd9365e750be56b09c530f20badab793d0f3cdfe\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-p267mwhv/wheels/b0/03/83/fb129110ddf28d31298b0731cd182d7d0f094935ae9bf8d4bc\n","Successfully built khaiii\n","Installing collected packages: khaiii\n","  Found existing installation: khaiii 0.4\n","    Uninstalling khaiii-0.4:\n","      Successfully uninstalled khaiii-0.4\n","Successfully installed khaiii-0.4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2xz4Z100Oqh3","colab_type":"text"},"source":["### 설치 후에 첨부한 manual을 khaiii/rsc/src/ 에 preanal.manual 파일을 넣은후 아래 코드를 실행한다. "]},{"cell_type":"code","metadata":{"id":"0EFCZfpCOfWZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1595760714774,"user_tz":-540,"elapsed":230048,"user":{"displayName":"이은열","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-vtFZD8JYZ-d5KFMLPO6e-yNOvrn2o_9v2sV-=s64","userId":"16187002805945562758"}},"outputId":"888657be-730c-41ae-abac-c9e834b0f9ac"},"source":["!cd /content/khaiii/build/ && make all\n","!cd /content/khaiii/build/ && make resource\n","!cd /content/khaiii/build/ && make install\n","!cd /content/khaiii/build/ && make package_python\n","!cd /content/khaiii/build/package_python && pip install ."],"execution_count":34,"outputs":[{"output_type":"stream","text":["[ 65%] Built target obj_khaiii\n","[ 69%] Built target khaiii\n","[ 76%] Built target bin_khaiii\n","[100%] Built target test_khaiii\n","INFO:root:preanal.manual\n","INFO:root:preanal.auto\n","INFO:root:trie saved: /content/khaiii/build/share/khaiii/preanal.tri\n","INFO:root:total nodes: 121165\n","INFO:root:expected size: 1938640\n","INFO:root:value saved: /content/khaiii/build/share/khaiii/preanal.val\n","INFO:root:total entries: 83523\n","INFO:root:expected size: 594956\n","Built target resource\n","[ 65%] Built target obj_khaiii\n","[ 69%] Built target khaiii\n","[ 76%] Built target bin_khaiii\n","[100%] Built target test_khaiii\n","\u001b[36mInstall the project...\u001b[0m\n","-- Install configuration: \"\"\n","-- Up-to-date: /usr/local/include/khaiii\n","-- Up-to-date: /usr/local/include/khaiii/khaiii_api.h\n","-- Up-to-date: /usr/local/include/khaiii/KhaiiiApi.hpp\n","-- Up-to-date: /usr/local/include/khaiii/khaiii_dev.h\n","-- Up-to-date: /usr/local/share/khaiii\n","-- Up-to-date: /usr/local/share/khaiii/hdn2tag.lin\n","-- Up-to-date: /usr/local/share/khaiii/errpatch.val\n","-- Up-to-date: /usr/local/share/khaiii/cnv2hdn.lin\n","-- Up-to-date: /usr/local/share/khaiii/config.json\n","-- Up-to-date: /usr/local/share/khaiii/restore.val\n","-- Up-to-date: /usr/local/share/khaiii/restore.one\n","-- Up-to-date: /usr/local/share/khaiii/errpatch.len\n","-- Up-to-date: /usr/local/share/khaiii/embed.bin\n","-- Up-to-date: /usr/local/share/khaiii/conv.3.fil\n","-- Up-to-date: /usr/local/share/khaiii/conv.2.fil\n","-- Up-to-date: /usr/local/share/khaiii/conv.4.fil\n","-- Installing: /usr/local/share/khaiii/preanal.val\n","-- Up-to-date: /usr/local/share/khaiii/restore.key\n","-- Up-to-date: /usr/local/share/khaiii/conv.5.fil\n","-- Up-to-date: /usr/local/share/khaiii/errpatch.tri\n","-- Installing: /usr/local/share/khaiii/preanal.tri\n","-- Up-to-date: /usr/local/lib/libkhaiii.so.0.4\n","-- Up-to-date: /usr/local/lib/libkhaiii.so.0\n","-- Up-to-date: /usr/local/lib/libkhaiii.so\n","-- Up-to-date: /usr/local/bin/khaiii\n","\u001b[36mRun CPack packaging tool for source...\u001b[0m\n","CPack: Create package using ZIP\n","CPack: Install projects\n","CPack: - Install directory: /content/khaiii\n","CPack: Create package\n","CPack: - package: /content/khaiii/build/khaiii-0.4.zip generated.\n","Built target package_python\n","Processing /content/khaiii/build/package_python\n","Building wheels for collected packages: khaiii\n","  Building wheel for khaiii (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for khaiii: filename=khaiii-0.4-cp36-none-any.whl size=22885666 sha256=3676419864c55df95597a7df9c80d23b7d7ea19888e397a45cd62e7d77bf4dc6\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-fes466b6/wheels/b0/03/83/fb129110ddf28d31298b0731cd182d7d0f094935ae9bf8d4bc\n","Successfully built khaiii\n","Installing collected packages: khaiii\n","  Found existing installation: khaiii 0.4\n","    Uninstalling khaiii-0.4:\n","      Successfully uninstalled khaiii-0.4\n","Successfully installed khaiii-0.4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_KaV9mSwQFk1","colab_type":"text"},"source":["## 드라이브 마운트 및 모델 실행을 위한 플레이리스트데이터 , 라이브러리 불러오기"]},{"cell_type":"code","metadata":{"id":"oG3hylxFO-C-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1595760715454,"user_tz":-540,"elapsed":658,"user":{"displayName":"이은열","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-vtFZD8JYZ-d5KFMLPO6e-yNOvrn2o_9v2sV-=s64","userId":"16187002805945562758"}},"outputId":"c61ee955-a792-4eb1-9f5e-443460d64955"},"source":["##드라이브 마운트\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":35,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HMCZvDejh7o9","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595760733053,"user_tz":-540,"elapsed":878,"user":{"displayName":"이은열","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-vtFZD8JYZ-d5KFMLPO6e-yNOvrn2o_9v2sV-=s64","userId":"16187002805945562758"}}},"source":["import pandas as pd"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"id":"8iXPM0YLea-D","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595760753321,"user_tz":-540,"elapsed":19341,"user":{"displayName":"이은열","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-vtFZD8JYZ-d5KFMLPO6e-yNOvrn2o_9v2sV-=s64","userId":"16187002805945562758"}}},"source":["## 파일 불러오기\n","train = pd.read_json('/content/drive/My Drive/arena_data/train.json') \n","val = pd.read_json('/content/drive/My Drive/arena_data/val.json') \n","test = pd.read_json('/content/drive/My Drive/arena_data/test.json') \n","song_meta = pd.read_json(\"/content/drive/My Drive/arena_data/song_meta.json\")\n"],"execution_count":38,"outputs":[]},{"cell_type":"code","metadata":{"id":"7nefTwiMk97e","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595760756077,"user_tz":-540,"elapsed":2754,"user":{"displayName":"이은열","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-vtFZD8JYZ-d5KFMLPO6e-yNOvrn2o_9v2sV-=s64","userId":"16187002805945562758"}}},"source":["train_for_popular = load_json('/content/drive/My Drive/arena_data/train.json')"],"execution_count":39,"outputs":[]},{"cell_type":"code","metadata":{"id":"8_7_3OMblVKK","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595760758707,"user_tz":-540,"elapsed":5382,"user":{"displayName":"이은열","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-vtFZD8JYZ-d5KFMLPO6e-yNOvrn2o_9v2sV-=s64","userId":"16187002805945562758"}}},"source":["# 인기곡)\n","_, song_mp = most_popular(train_for_popular, \"songs\", 200)"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"id":"9B0jMFgWPE6A","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595760764498,"user_tz":-540,"elapsed":650,"user":{"displayName":"이은열","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-vtFZD8JYZ-d5KFMLPO6e-yNOvrn2o_9v2sV-=s64","userId":"16187002805945562758"}}},"source":["import json\n","import io\n","import re\n","import os\n","import distutils.dir_util\n","import numpy as np\n","import scipy.sparse as spr\n","import pickle\n","from collections import Counter\n","\n","from gensim.models import Word2Vec\n","from gensim.models.keyedvectors import *\n","from sklearn.metrics.pairwise import cosine_similarity\n","from tqdm import tqdm\n","def remove_seen(seen, l):\n","    seen = set(seen)\n","    return [x for x in l if not (x in seen)]\n","\n","def write_json(data, fname):\n","    def _conv(o):\n","        if isinstance(o, np.int64) or isinstance(o, np.int32):\n","            return int(o)\n","        raise TypeError\n","\n","    parent = os.path.dirname(fname)\n","    distutils.dir_util.mkpath(\"./arena_data/\" + parent)\n","    with io.open(\"./arena_data/\" + fname, \"w\", encoding=\"utf8\") as f:\n","        json_str = json.dumps(data, ensure_ascii=False, default=_conv)\n","        f.write(json_str)\n","\n","\n","def load_json(fname):\n","    with open(fname, encoding='utf8') as f:\n","        json_obj = json.load(f)\n","\n","    return json_obj\n","\n","\n","def debug_json(r):\n","    print(json.dumps(r, ensure_ascii=False, indent=4))\n","\n","def most_popular(playlists, col, topk_count):\n","    c = Counter()\n","\n","    for doc in playlists:\n","        c.update(doc[col])\n","\n","    topk = c.most_common(topk_count)\n","    return c, [k for k, v in topk]\n","\n","class CustomEvaluator:\n","    def _idcg(self, l):\n","        return sum((1.0 / np.log(i + 2) for i in range(l)))\n","\n","    def __init__(self):\n","        self._idcgs = [self._idcg(i) for i in range(101)]\n","\n","    def _ndcg(self, gt, rec):\n","        dcg = 0.0\n","        for i, r in enumerate(rec):\n","            if r in gt:\n","                dcg += 1.0 / np.log(i + 2)\n","\n","        return dcg / self._idcgs[len(gt)]\n","\n","    def _eval(self, gt_fname, rec_fname):\n","        gt_playlists = load_json(gt_fname)\n","        gt_dict = {g[\"id\"]: g for g in gt_playlists}\n","        rec_playlists = load_json(rec_fname)\n","        \n","        music_ndcg = 0.0\n","        tag_ndcg = 0.0\n","\n","        for rec in rec_playlists:\n","            gt = gt_dict[rec[\"id\"]]\n","            music_ndcg += self._ndcg(gt[\"songs\"], rec[\"songs\"][:100])\n","            tag_ndcg += self._ndcg(gt[\"tags\"], rec[\"tags\"][:10])\n","\n","        music_ndcg = music_ndcg / len(rec_playlists)\n","        tag_ndcg = tag_ndcg / len(rec_playlists)\n","        score = music_ndcg * 0.85 + tag_ndcg * 0.15\n","\n","        return music_ndcg, tag_ndcg, score\n","\n","    def evaluate(self, gt_fname, rec_fname):\n","        try:\n","            music_ndcg, tag_ndcg, score = self._eval(gt_fname, rec_fname)\n","            print(f\"Music nDCG: {music_ndcg:.6}\")\n","            print(f\"Tag nDCG: {tag_ndcg:.6}\")\n","            print(f\"Score: {score:.6}\")\n","        except Exception as e:\n","            print(e)\n","\n","\n","\n","## --------------------------------------------형태소 분석기관련 라이브러리\n","from typing import *\n","import matplotlib.pyplot as plt; plt.rcdefaults()\n","from khaiii import KhaiiiApi  # khaiii 레포는 https://github.com/kakao/khaiii 이쪽\n","tokenizer = KhaiiiApi()\n","import multiprocessing\n","# from gensim.models import FastText\n","\n","# from gensim.models.fasttext import FastText as FT_gensim\n","\n"],"execution_count":41,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vVp_imwLP4sE","colab_type":"text"},"source":["## 플레이리스트 데이터 토크나이징 하기\n"]},{"cell_type":"code","metadata":{"id":"ef44Pqf8Po3_","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595760784607,"user_tz":-540,"elapsed":12099,"user":{"displayName":"이은열","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-vtFZD8JYZ-d5KFMLPO6e-yNOvrn2o_9v2sV-=s64","userId":"16187002805945562758"}}},"source":["def re_sub(series: pd.Series) -> pd.Series:\n","    series = series.str.replace(pat=r'[ㄱ-ㅎ]', repl=r'', regex=True)  # ㅋ 제거용\n","    series = series.str.replace(pat=r'[^\\w\\s]', repl=r'', regex=True)  # 특수문자 제거\n","    series = series.str.replace(pat=r'[ ]{2,}', repl=r' ', regex=True)  # 공백 제거\n","    series = series.str.replace(pat=r'[\\u3000]+', repl=r'', regex=True)  # u3000 제거\n","    return series\n","\n","def flatten(list_of_list : List) -> List:\n","    flatten = [j for i in list_of_list for j in i]\n","    return flatten\n","\n","def get_token(title: str, tokenizer)-> List[Tuple]:\n","  # -> 어노테이션을 뜻하며 리턴할떄 튜플이 return된다는걸로 일단 추정하고 넘어간다\n","  if len(title)== 0 or title== ' ':  # 제목이 공백인 경우 tokenizer에러 발생\n","      return []\n","  result = tokenizer.analyze(title)\n","\n","  result_token = []\n","  for split in result:\n","    for morph in split.morphs:\n","      if len(morph.lex) > 1:\n","        result_token.append((morph.lex, morph.tag))\n","      elif len(morph.lex) == 1:\n","          # 원래 한글자단어는 다 삭제시키는데 특성으로 사용할만한 한글자 단어만 추출해서 사용하도록함\n","        if morph.lex == \"비\" or morph.lex == \"밤\" or morph.lex == \"록\" or morph.lex == \"락\"or morph.lex == \"봄\"or morph.lex == \"춤\"or morph.lex == \"팝\" or morph.lex == \"넬\" or morph.lex == \"책\":\n","          result_token.append((morph.lex, morph.tag))\n","  return result_token\n","\n","\n","def get_all_tags(df) -> List:\n","    tag_list = df['tags'].values.tolist() \n","    tag_list = flatten(tag_list)\n","    return tag_list\n","\n","def get_gns(target_song):\n","    if len(song_and_gn) <= 0:\n","        # song_and_gnd은 곡의 아이디 = 대분류 장르로 매핑되어있는 오브젝트이다. {곡번호:장르 ,곡번호:장르 ,}로 되어있음\n","        # for song in song_meta: 은 곡의 아이디와 장르를 새로운 오브젝트로 매핑하는 코드\n","        for song in song_meta:#함수화 하지않을거면 이부분 따로 전역으로 뺴서 써도됨.\n","            #밑의 if문은 장르를 한글로 집어넣기위한 예외처리 \n","            song_and_gn[song['id']] = song['song_gn_gnr_basket']\n","    res = song_and_gn.get(int(target_song))\n","    return res\n","\n","\n","with open(os.path.join('/content/drive/My Drive/song_meta.json'), encoding=\"utf-8\") as f:\n","    song_meta = json.load(f)\n","song_and_gn = {}\n","def _get_gns(target_song):\n","    if len(song_and_gn) <= 0:\n","        # song_and_gnd은 곡의 아이디 = 대분류 장르로 매핑되어있는 오브젝트이다. {곡번호:장르 ,곡번호:장르 ,}로 되어있음\n","        # for song in song_meta: 은 곡의 아이디와 장르를 새로운 오브젝트로 매핑하는 코드\n","        for song in song_meta:#함수화 하지않을거면 이부분 따로 전역으로 뺴서 써도됨.\n","            song_and_gn[song['id']] = song['song_gn_gnr_basket']\n","    res = song_and_gn.get(int(target_song))\n","    return res\n","# print(_get_gns(13542)) >>>>  ['GN2500', 'GN0200']\n"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"id":"SK4aCI3TQWPV","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595760860849,"user_tz":-540,"elapsed":87124,"user":{"displayName":"이은열","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-vtFZD8JYZ-d5KFMLPO6e-yNOvrn2o_9v2sV-=s64","userId":"16187002805945562758"}}},"source":["using_pos = ['NNG','SL','NNP','XR']  # 일반 명사, 외국어, 고유 명사, 어근  \n","no_use_items = ['노래' , '플레이리스트' , '음악' ,'주관','요즘', '모음' , '나를' , '나만' ,'나에' , '내맘', '가요' , '초급' , '함께' , '너무' , '좋은' , '테스트' ,'가장' , '최고' , '명곡'\n",",'진짜' , '감성' , '매력' , '수록곡' , '그냥' , '없이' , '후기' , '느낌' , '오프닝' , '분위기', '삽입곡' ,'강추' ,'일때', '발매곡' , '입문' , '다재다능' ,'부제하',\n","'자외' , '비트' , '고고씽' , '디편','사이','특별','순간','완전','BUT','세상','자국','저릿','THE','리메이','얘기','개인','선곡','코로','추천곡','대변','사람','멜로디'\n","'방법', '질때' , '분위기','정규앨범','가사','아실테죠','입장','위주','극복','사람','연습곡','은곡' , '컬렉션','TOP','베스트','드루','모두','이번']\n","\n","val['plylst_title'] = re_sub(val['plylst_title'])\n","val.loc[:, 'ply_token'] = val['plylst_title'].map(lambda x: get_token(x, tokenizer))\n","val['ply_token'] = val['ply_token'].map(lambda x: list(filter(lambda x: x[1] in using_pos, x)))\n","val['ply_token'] = val['ply_token'].map(lambda x: list(filter(lambda x: not(x[0]) in no_use_items, x)))\n","val['ply_token'] = val['ply_token'].map(lambda x : [\"_\"+i[0] for i in x] )\n","\n","test['plylst_title'] = re_sub(test['plylst_title'])\n","test.loc[:, 'ply_token'] = test['plylst_title'].map(lambda x: get_token(x, tokenizer))\n","test['ply_token'] = test['ply_token'].map(lambda x: list(filter(lambda x: x[1] in using_pos, x)))\n","test['ply_token'] = test['ply_token'].map(lambda x: list(filter(lambda x: not(x[0]) in no_use_items, x)))\n","test['ply_token'] = test['ply_token'].map(lambda x : [\"_\"+i[0] for i in x] )\n","\n","train['plylst_title'] = re_sub(train['plylst_title'])\n","train.loc[:, 'ply_token'] = train['plylst_title'].map(lambda x: get_token(x, tokenizer))\n","train['ply_token'] = train['ply_token'].map(lambda x: list(filter(lambda x: x[1] in using_pos, x)))\n","train['ply_token'] = train['ply_token'].map(lambda x: list(filter(lambda x: not(x[0]) in no_use_items, x)))\n","train['ply_token'] = train['ply_token'].map(lambda x : [\"_\"+i[0] for i in x] )"],"execution_count":43,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lTgLtbCyTJ7p","colab_type":"text"},"source":["### 변환한 데이터를 저장"]},{"cell_type":"code","metadata":{"id":"0DxHF6UMTGP6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1595761229539,"user_tz":-540,"elapsed":368669,"user":{"displayName":"이은열","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-vtFZD8JYZ-d5KFMLPO6e-yNOvrn2o_9v2sV-=s64","userId":"16187002805945562758"}},"outputId":"7771eaea-46ef-4ba1-d76e-5af3f7e0d35e"},"source":["res = []\n","for idx in tqdm(val.index):\n","    id = val.loc[idx,\"id\"]\n","    songs = val[val[\"id\"]==id][\"songs\"].values[0]\n","    tag = val[val[\"id\"]==id][\"tags\"].values[0]\n","    token = val[val[\"id\"]==id][\"ply_token\"].values[0]\n","        \n","    res.append({\n","            \"id\": id,\n","                \"songs\": songs,\n","                \"tags\": tag,\n","            \"ply_token\":token\n","         })\n","write_json(res, \"val_token.json\")\n","res = []\n","for idx in tqdm(train.index):\n","    id = train.loc[idx,\"id\"]\n","    songs = train[train[\"id\"]==id][\"songs\"].values[0]\n","    tag = train[train[\"id\"]==id][\"tags\"].values[0]\n","    token = train[train[\"id\"]==id][\"ply_token\"].values[0]   \n","    res.append({\n","            \"id\": id,\n","                \"songs\": songs,\n","                \"tags\": tag,\n","                \"ply_token\":token\n","         })\n","##합쳐진 18000개의 VAL 저장\n","write_json(res, \"train_token.json\")\n","res = []\n","for idx in tqdm(test.index):\n","    id = test.loc[idx,\"id\"]\n","    songs = test[test[\"id\"]==id][\"songs\"].values[0]\n","    tag = test[test[\"id\"]==id][\"tags\"].values[0]\n","    token = test[test[\"id\"]==id][\"ply_token\"].values[0]    \n","    res.append({\n","            \"id\": id,\n","                \"songs\": songs,\n","                \"tags\": tag,\n","                \"ply_token\":token\n","         })\n","write_json(res, \"test_token.json\")"],"execution_count":44,"outputs":[{"output_type":"stream","text":["100%|██████████| 23015/23015 [00:55<00:00, 414.32it/s]\n","100%|██████████| 115071/115071 [04:42<00:00, 406.74it/s]\n","100%|██████████| 10740/10740 [00:25<00:00, 419.96it/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"KBZX_FTxQlbS","colab_type":"text"},"source":["## 토크나이징한 작업물을 word2vec을 이용하여 임베딩하기\n"]},{"cell_type":"markdown","metadata":{"id":"txkUZla3r9kz","colab_type":"text"},"source":["### 임베딩작업후 나오는 모델은  총 2가지\n"]},{"cell_type":"markdown","metadata":{"id":"hXOJFIUy-nb0","colab_type":"text"},"source":["#### 곡과 태그 , 플레이리스트 제목 토큰화화여  임베딩하는 모델"]},{"cell_type":"code","metadata":{"id":"ClmDtnI5Amq-","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595765128764,"user_tz":-540,"elapsed":856,"user":{"displayName":"이은열","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-vtFZD8JYZ-d5KFMLPO6e-yNOvrn2o_9v2sV-=s64","userId":"16187002805945562758"}}},"source":["error2 = []\n","error= 0"],"execution_count":47,"outputs":[]},{"cell_type":"code","metadata":{"id":"_g6hPmVrQXhL","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595765129168,"user_tz":-540,"elapsed":906,"user":{"displayName":"이은열","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-vtFZD8JYZ-d5KFMLPO6e-yNOvrn2o_9v2sV-=s64","userId":"16187002805945562758"}}},"source":["class PlaylistEmbedding:\n","    ## init 에서 train , val 데이터를 주고 , Word2vec사용시 선택값들을 넣어준다.\n","    def __init__(self, FILE_PATH):\n","        with open(os.path.join(FILE_PATH, 'arena_data/train_token.json'), encoding=\"utf-8\") as f:\n","            self.train = json.load(f)\n","        with open(os.path.join(FILE_PATH, 'arena_data/val_token.json'), encoding=\"utf-8\") as f:\n","            self.val = json.load(f)\n","        with open(os.path.join(FILE_PATH, 'arena_data/test_token.json'), encoding=\"utf-8\") as f:\n","            self.test = json.load(f)\n","        self.most_results = pd.read_json(\"arena_data/most_popular.json\")\n","\n","\n","        self.FILE_PATH = FILE_PATH  \n","        self.min_count = 2\n","        self.size = 128\n","        self.window = 210\n","        self.sg = 5\n","        self.p2v_model = WordEmbeddingsKeyedVectors(128)\n","       \n","        \n","        \n","      \n","            \n","    ## train 데이터와 val 데이터를 합친 후 그 플레이리스트의 id값에 곡들을 매칭시킨다.\n","  \n","    def get_dic(self, train, val,test):\n","        song_dic = {}\n","        tag_dic = {}\n","        data = train+val+test\n","        for q in tqdm(data):\n","            song_dic[str(q['id'])] = q['songs']\n","            tag_dic[str(q['id'])] = q['tags']\n","        self.song_dic = song_dic\n","        self.tag_dic = tag_dic\n","        ## 플레이리스트마다 곡과 태그들을 합쳐 리스트화 하는데 합친 값이 0 즉 곡과 태그들이 전혀없는경우는 제외한다.\n","        total = list(map(lambda x: list(map(str, x['songs'])) + list(x['tags'])+ list(x['ply_token']), data))\n","        total = [x for x in total if len(x)>1]\n","        self.total = total\n","        \n","        #Word2vec메소드 실행 (곡과 태그를 함께 임베딩)\n","    def get_w2v(self, total, min_count, size, window, sg):\n","        print(\"학습시작\")\n","        w2v_model = Word2Vec(total, min_count = min_count, size = size, window = window, sg = sg,workers=6)\n","        w2v_model.wv.save_word2vec_format('token_model_send_dimension128_token') # 모델 저장\n","        # loaded_model = KeyedVectors.load_word2vec_format(\"token_model_send_dimension128_token\")\n","        # w2v_model = loaded_model\n","        self.w2v_model = w2v_model\n","        print(\"학습완료\")\n","    ## \n","    def update_p2v(self, train, val,test, w2v_model):\n","        ID = []   \n","        vec = []\n","        for q in tqdm(train+val+test):\n","            tmp_vec = 0\n","            \n","            ##곡이 1개 이상인 경우에 곡과 태그의 백터값을 구한후 모두 더해 플레이리스트의 벡터,\n","            ##즉 플레이리스트의 백터를 구한다 (tmp_vec)\n","            ##여기서 나는 곡이 1개이상인  val플레이리스트기준으로 하였기때문에 왠만하면 if문 무조건넘어감\n","            for song in q['songs']+q['tags']+q['ply_token']:\n","                        \n","                    try:\n","                        \n","                        tmp_vec += w2v_model.wv.get_vector(str(song))\n","                        not_error.append(str(song))\n","                    except:\n","                        \n","                        error2.append(str(song))\n","                        pass\n","            if type(tmp_vec)!=int:\n","                ID.append(str(q['id']))    \n","                vec.append(tmp_vec)\n","       #구한 플레이리스트 백터리스트를 p2vmodel에 저장한다.\n","        self.p2v_model.add(ID, vec)\n","        print(\"업데이트 완료\")\n","    \n","    ## 플레이리스트 유사도를 구한후 결과를 내는 메소드\n","    def get_result(self, p2v_model, song_dic, tag_dic, most_results, val):\n","        answers = []\n","        error = 0\n","        for n, q in tqdm(enumerate(val), total = len(val)):\n","            try:\n","                #문제플레이리스트와 가장 유사한 플레이리스트 60개의 ID를 뽑는다.\n","\n","                most_id = [x[0] for x in p2v_model.most_similar(str(q['id']), topn=60)]\n","\n","                get_song = []\n","                get_tag = []\n","                #플레이리스트ID를 이용하여 안에 들어있는 곡과 태그를 전부 꺼낸다.\n","                for ID in most_id:\n","                    get_song += song_dic[ID]\n","                    get_tag += tag_dic[ID]\n","                 \n","                #꺼내진 리스트를 value_count를 통하여 많이 중복된 순으로 곡을200개 가져온다.\n","                get_song = list(pd.value_counts(get_song)[:200].index)\n","                get_tag = list(pd.value_counts(get_tag)[:20].index)\n","                \n","                ##중복을 제거한 후 100개까지 가져온다.\n","                answers.append({\n","                    \"id\": q[\"id\"],\n","                    \"songs\": remove_seen(q[\"songs\"], get_song)[:100],\n","                    \"tags\": remove_seen(q[\"tags\"], get_tag)[:10],\n","                })\n","                #예외가 발생할 경우 results 데이터의 곡과 태그들을 집어넣는다\n","                #여기서 발생할 수 있는 예외는 곡이 0개일때를 뜻한다.\n","                #그러므로 여기로 오면 문제가 있는거임 (곡이있는 리스트만 불러왔기때문)\n","            except:\n","                error = error+1\n","                \n","                answers.append({\n","                  \"id\":most_results[most_results[\"id\"]==q[\"id\"]][\"id\"].values[0],\n","                  \"songs\": most_results[most_results[\"id\"]==q[\"id\"]][\"songs\"].values[0],\n","                  \"tags\":most_results[most_results[\"id\"]==q[\"id\"]][\"tags\"].values[0],\n","                }) \n","        # 여기서 만약 중복제외곡이 100곡이 안된다면 most_results에서 곡을 더 가져와서 100개를 맞춘다.\n","        #태그도 마찬가지\n","        error3 = 0\n","        for n, q in enumerate(answers):\n","            if len(q['songs'])!=100:\n","                ##곡이 100곡이 안됬을경우더하는값\n","                error3 = error3+1\n","                answers[n]['songs'] += remove_seen(q['songs'], self.most_results[most_results[\"id\"]==q[\"id\"]][\"songs\"].values[0])[:100-len(q['songs'])]\n","            if len(q['tags'])!=10:\n","                answers[n]['tags'] += remove_seen(q['tags'], self.most_results[most_results[\"id\"]==q[\"id\"]][\"tags\"].values[0])[:10-len(q['tags'])]\n","        print(str(error)+\"결과오류발생횟수\")\n","        print(str(error3)+\"곡 오류발생횟수(100곡이안되었을경우)\")\n","        self.answers = answers\n","    \n","    def run(self):\n","        self.get_dic(self.train, self.val,self.test)\n","        self.get_w2v(self.total, self.min_count, self.size, self.window, self.sg)\n","        self.update_p2v(self.train, self.val,self.test, self.w2v_model)\n","        self.get_result(self.p2v_model, self.song_dic, self.tag_dic, self.most_results, self.test)\n","        write_json(self.answers, 'embedding_model_tag.json')\n","\n","        "],"execution_count":48,"outputs":[]},{"cell_type":"code","metadata":{"id":"SSc-cz0MSSM3","colab_type":"code","colab":{}},"source":["FILE_PATH = ''\n","U_space = PlaylistEmbedding(FILE_PATH)\n","U_space.run()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9_kQSe2O-say","colab_type":"text"},"source":["### 곡, 태그 ,제목 토크나이즈 한것을 임베딩하는 모델"]},{"cell_type":"code","metadata":{"id":"Z5GzLkBBvcd2","colab_type":"code","colab":{}},"source":["class PlaylistEmbedding:\n","    ## init 에서 train , val 데이터를 주고 , Word2vec사용시 선택값들을 넣어준다.\n","    def __init__(self, FILE_PATH):\n","        with open(os.path.join(FILE_PATH, 'train_token.json'), encoding=\"utf-8\") as f:\n","            self.train = json.load(f)\n","        with open(os.path.join(FILE_PATH, 'val_token.json'), encoding=\"utf-8\") as f:\n","            self.val = json.load(f)\n","        with open(os.path.join(FILE_PATH, 'test_token.json'), encoding=\"utf-8\") as f:\n","            self.test = json.load(f)\n","        self.most_results = pd.read_json(\"arena_data/most_popular.json\")\n","\n","\n","        self.FILE_PATH = FILE_PATH  #('arena_data/')\n","        self.min_count = 2\n","        self.size = 128\n","        self.window = 210\n","        self.sg = 5\n","        self.p2v_model = WordEmbeddingsKeyedVectors(self.size)\n","       \n","        \n","        \n","      \n","            \n","    ## train 데이터와 val 데이터를 합친 후 그 플레이리스트의 id값에 곡들을 매칭시킨다.\n","  \n","    def get_dic(self, train, val,test):\n","        song_dic = {}\n","        tag_dic = {}\n","        data = train+val+test\n","        for q in tqdm(data):\n","            song_dic[str(q['id'])] = q['songs']\n","            tag_dic[str(q['id'])] = q['tags']\n","        self.song_dic = song_dic\n","        self.tag_dic = tag_dic\n","        ## 플레이리스트마다 곡과 태그들을 합쳐 리스트화 하는데 합친 값이 0 즉 곡과 태그들이 전혀없는경우는 제외한다.\n","        total = list(map(lambda x: list(map(str, x['songs'])) + list(x['tags']), data))\n","        total = [x for x in total if len(x)>1]\n","        self.total = total\n","        \n","        #Word2vec메소드 실행 (곡과 태그를 함께 임베딩)\n","    def get_w2v(self, total, min_count, size, window, sg):\n","        print(\"학습시작\")\n","        w2v_model = Word2Vec(total, min_count = min_count, size = size, window = window, sg = sg,workers=6)\n","        w2v_model.wv.save_word2vec_format('getsong_2_topn12_dimension128') # 모델 저장\n","#         loaded_model = KeyedVectors.load_word2vec_format(\"token_model_send_iter10\")\n","#         w2v_model = loaded_model\n","        self.w2v_model = w2v_model\n","        print(\"학습완료\")\n","    ## \n","    def update_p2v(self, train, val,test, w2v_model):\n","        ID = []   \n","        vec = []\n","        for q in tqdm(train+val+test):\n","            tmp_vec = 0\n","            \n","            ##곡이 1개 이상인 경우에 곡과 태그의 백터값을 구한후 모두 더해 플레이리스트의 벡터,\n","            ##즉 플레이리스트의 백터를 구한다 (tmp_vec)\n","            ##여기서 나는 곡이 1개이상인  val플레이리스트기준으로 하였기때문에 왠만하면 if문 무조건넘어감\n","            for song in q['songs']+q['tags']:\n","                        \n","                    try:\n","                        \n","                        tmp_vec += w2v_model.wv.get_vector(str(song))\n","                        not_error.append(str(song))\n","                    except:\n","                        \n","                        error2.append(str(song))\n","                        pass\n","            if type(tmp_vec)!=int:\n","                ID.append(str(q['id']))    \n","                vec.append(tmp_vec)\n","       #구한 플레이리스트 백터리스트를 p2vmodel에 저장한다.\n","        self.p2v_model.add(ID, vec)\n","        print(\"업데이트 완료\")\n","    \n","    ## 플레이리스트 유사도를 구한후 결과를 내는 메소드\n","    def get_result(self, p2v_model, song_dic, tag_dic, most_results, val):\n","        answers = []\n","        error = 0\n","        for n, q in tqdm(enumerate(val), total = len(val)):\n","            try:\n","                #문제플레이리스트와 가장 유사한 플레이리스트 200개의 ID를 뽑는다.\n","\n","                most_id = [x[0] for x in p2v_model.most_similar(str(q['id']), topn=12)]\n","\n","                get_song = []\n","                get_tag = []\n","                #플레이리스트ID를 이용하여 안에 들어있는 곡과 태그를 전부 꺼낸다.\n","                for ID in most_id:\n","                    get_song += song_dic[ID]\n","                    get_tag += tag_dic[ID]\n","                 \n","                #꺼내진 리스트를 value_count를 통하여 많이 중복된 순으로 곡을200개 가져온다.\n","                get_song = list(pd.value_counts(get_song)[:200].index)\n","#                 val_update_time = q[\"updt_date\"]\n","#                 get_song_update = []\n","#                 val_date = int(val_update_time.split(\"-\")[0]+val_update_time.split(\"-\")[1]+val_update_time.split(\"-\")[2].split(\" \")[0])\n","#                 for songs in get_song:\n","#                     song_date = song_meta[song_meta[\"id\"]==songs][\"issue_date\"].values[0]\n","#                     if val_date-song_date>0:\n","#                         get_song_update.append(songs)\n","#                     else:\n","#                         print(\"날짜비정상\")\n","#                         print(val_update_time+\"=>\"+str(val_date)+\"-\"+str(song_date))\n","                get_tag = list(pd.value_counts(get_tag)[:20].index)\n","                \n","                ##중복을 제거한 후 100개까지 가져온다.\n","                answers.append({\n","                    \"id\": q[\"id\"],\n","                    \"songs\": remove_seen(q[\"songs\"], get_song)[:100],\n","                    \"tags\": remove_seen(q[\"tags\"], get_tag)[:10],\n","                })\n","                #예외가 발생할 경우 results 데이터의 곡과 태그들을 집어넣는다\n","                #여기서 발생할 수 있는 예외는 곡이 0개일때를 뜻한다.\n","                #그러므로 여기로 오면 문제가 있는거임 (곡이있는 리스트만 불러왔기때문)\n","            except:\n","                error = error+1\n","                \n","                answers.append({\n","                  \"id\":most_results[most_results[\"id\"]==q[\"id\"]][\"id\"].values[0],\n","                  \"songs\": most_results[most_results[\"id\"]==q[\"id\"]][\"songs\"].values[0],\n","                  \"tags\":most_results[most_results[\"id\"]==q[\"id\"]][\"tags\"].values[0],\n","                }) \n","        # 여기서 만약 중복제외곡이 100곡이 안된다면 most_results에서 곡을 더 가져와서 100개를 맞춘다.\n","        #태그도 마찬가지\n","        error3 = 0\n","        for n, q in enumerate(answers):\n","            if len(q['songs'])!=100:\n","                ##곡이 100곡이 안됬을경우더하는값\n","                error3 = error3+1\n","                answers[n]['songs'] += remove_seen(q['songs'], self.most_results[most_results[\"id\"]==q[\"id\"]][\"songs\"].values[0])[:100-len(q['songs'])]\n","            if len(q['tags'])!=10:\n","                answers[n]['tags'] += remove_seen(q['tags'], self.most_results[most_results[\"id\"]==q[\"id\"]][\"tags\"].values[0])[:10-len(q['tags'])]\n","        print(str(error)+\"결과오류발생횟수\")\n","        print(str(error3)+\"곡 오류발생횟수(100곡이안되었을경우)\")\n","        self.answers = answers\n","    \n","    def run(self):\n","        self.get_dic(self.train, self.val,self.test)\n","        self.get_w2v(self.total, self.min_count, self.size, self.window, self.sg)\n","        self.update_p2v(self.train, self.val,self.test, self.w2v_model)\n","        self.get_result(self.p2v_model, self.song_dic, self.tag_dic, self.most_results, self.test)\n","        write_json(self.answers, 'embedding_model_has_song.json')\n","\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"guivlyxIvdcU","colab_type":"code","colab":{}},"source":["FILE_PATH = ''\n","U_space = PlaylistEmbedding(FILE_PATH)\n","U_space.run()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oNfqa98dVg7O","colab_type":"text"},"source":["## 곡이 없는부분을 뽑기위한 모델"]},{"cell_type":"code","metadata":{"id":"ItxNlr-_VkVv","colab_type":"code","colab":{}},"source":["# ★★★★★★★★★★★★★★★★★★★ 경로 체크 \n","train = pd.read_json('/content/drive/My Drive/arena_data/train.json') \n","val = pd.read_json('/content/drive/My Drive/arena_data/val.json') \n","with open(os.path.join('/content/drive/My Drive/arena_data/val.json'), encoding=\"utf-8\") as f: \n","  val_json = json.load(f)\n","with open(os.path.join('/content/drive/My Drive/arena_data/results.json'), encoding=\"utf-8\") as f:\n","  most_results = json.load(f)\n","test = pd.read_json('/content/drive/My Drive/arena_data/test.json')\n","with open(os.path.join('/content/drive/My Drive/arena_data/test.json'), encoding=\"utf-8\") as f:\n","  test_json = json.load(f)\n","song_meta = pd.read_json('/content/drive/My Drive/arena_data/song_meta.json')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xGqJNUZIqbpE","colab_type":"code","colab":{}},"source":["# word_book = 데이터 프레임 을 만드는 과정 - 약 15분 소요\n","# word_book 은 데이터셋(현재코드는 트레인)에서 \n","# word_book = 단어 | 단어출현회수 | 단어가 출현한 플레이리스트 | 플레이리스트의 개수 | 곡(곡넘버 , 곡출현횟수 , 해당 단어의 곡이 출현할 퍼센트)\n","\n","# 태그는 배열로 저장되어있어서 태그컬럼을 스트링으로 만들어 주는 포문\n","for i in train.index:\n","  tmp_str = \"\"\n","  for k in train.loc[i,'tags']:\n","    tmp_str = tmp_str + \" \" + k\n","  train.loc[i,'tags_token'] = tmp_str\n","\n","# 트레인데이터의 제목 과 태그를 정제 후 스트링 \n","train['plylst_title'] = re_sub(train['plylst_title'])\n","train.loc[:, 'ply_token'] = train['plylst_title'].map(lambda x: get_token(x, tokenizer))\n","\n","train['tags_token'] = re_sub(train['tags_token'])\n","train.loc[:, 'tags_token'] = train['tags_token'].map(lambda x: get_token(x, tokenizer))\n","\n","using_pos = ['NNG','SL','NNP','XR']  # 일반 명사, 외국어, 고유 명사, 어근  \n","no_use_items = ['노래' , '플레이리스트' , '음악' ,'주관','요즘', '모음' , '나를' , '나만' ,'나에' , '내맘', '가요' , '초급' , '함께' , '너무' , '좋은' , '테스트' ,'가장' , '최고' , '명곡'\n",",'진짜' , '감성' , '매력' , '수록곡' , '그냥' , '없이' , '후기' , '느낌' , '오프닝' , '분위기', '삽입곡' ,'강추' ,'일때', '발매곡' , '입문' , '다재다능' ,'부제하','세대'\n","'자외' , '비트' , '고고씽' , '디편','사이','특별','순간','완전','BUT','세상','자국','저릿','THE','리메이','얘기','개인','선곡','코로','추천곡','대변','사람','멜로디'\n","'방법', '질때' , '분위기','정규앨범','가사','아실테죠','입장','위주','극복','사람','연습곡','은곡' , '컬렉션','TOP','베스트','드루','모두','이번','빠돌이','프로듀서','타이틀','고음불'\n","'다양','장르','소장','주간','마무리','메들리','월간','종합선물세트','결산','고기구' ,'느낌나','확찐','확빠', '필요' ,'집순']\n","train['ply_token'] = train['ply_token'].map(lambda x: list(filter(lambda x: x[1] in using_pos, x)))\n","train['ply_token'] = train['ply_token'].map(lambda x: list(filter(lambda x: not(x[0]) in no_use_items, x)))\n","train['tags_token'] = train['tags_token'].map(lambda x: list(filter(lambda x: x[1] in using_pos, x)))\n","train['tags_token'] = train['tags_token'].map(lambda x: list(filter(lambda x: not(x[0]) in no_use_items, x)))\n","train_token = train[['tags_token','ply_token']].values.tolist()\n","\n","train_tags_title_list = [v  for v in train_token]\n","result_token_items = [] #하나의 리스트에 태그랑 토큰된제목이 들어가 있는 배열\n","for i , v in enumerate(train_tags_title_list):\n","  result_token_items.append([])\n","  for k in v:\n","    for j in k:\n","      if type(j) == str:\n","        result_token_items[i].append(j)\n","      else: # 데이터 타입이 튜플일 경우를 엘스\n","        result_token_items[i].append(j[0])\n","\n","convert_str_token_list = []\n","for i , v in enumerate(result_token_items):\n","  convert_str_token_list.append([])\n","  append_word = \"\"\n","  for k in v:\n","    append_word = append_word +\" \"+str(k)\n","  convert_str_token_list[i].append(append_word)\n","\n","#단어목록(단어 , 등장횟수)로 만들고 리스트로 변환후 데이터프레임에 word_book에 추가함\n","tag_title_token_list = [element for array in result_token_items for element in array]\n","token_temp = Counter(tag_title_token_list)\n","tokens, cnt = list(token_temp.keys()), list(token_temp.values())\n","\n","# ★ 단어장으로 사용할 단어 데이터프레임\n","word_book = pd.DataFrame({'word':tokens , 'word_cnt':cnt})\n","word_book['playlist'] = np.empty((len(word_book), 0)).tolist() #플레이리스트에 빈배열로 넣어놓고 내용물은 추가하는방식으로 사용\n","word_book['playlist_cnt'] = 0 \n","\n","# 각 단어가 어디 플레이리스트에 맵핑 되어잇는지 확인하는 코드\n","token_dic_with_playlist = {} \n","\n","playlist_title_dic_with_id = {} \n","# 플레이리스트 아이디에 플레이리스트 스트링을 매핑 시켜놓았음 find문으로 단어가 포함되어있는지 아닌지를 확인할 예정\n","for i in train.index:\n","  playlist_title_dic_with_id[train.loc[i,'id']]=train.loc[i,'plylst_title']\n","\n","tmp_token_with_playlist = {} # 단어 : 플레이리스트 목록 을 키로 만들어놓고 word_book에 매핑하도록함\n","for i in tqdm(tokens):\n","  tmp_token_with_playlist[i] = []\n","  for (key, elem) in playlist_title_dic_with_id.items():\n","    if i in elem:\n","      tmp_token_with_playlist[i].append(key)\n","\n","print(\"word_book 데이터프레임 추가하는 작업중..\")\n","word_book[\"playlist\"] = word_book[\"word\"].map(lambda x : tmp_token_with_playlist[str(x)])\n","tmp_cnt = [len(word_book.loc[i]['playlist']) for i in word_book.index]\n","word_book[\"playlist_cnt\"] = tmp_cnt\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m6u1-Ni5qhmp","colab_type":"code","colab":{}},"source":["# word_book['songs_cnt_appear_percent']을 추가하는 코드\n","# 키값으론 단어(index)기준으로 (곡넘버 , 곡 출현횟수 , 단어에 곡이 출현한 확률)이 저장되어잇다\n","# 곡이 30프로 이상 출현 하면 정답으로 제출되며 추가로 벡터값(w2v)으로 시밀리러티한 곡들도 추가로 가져온다\n","\n","songs_dic ={} #플레이리스트 아이디: 곡이랑 맵핑\n","for i in train.index:\n","  songs_dic[train.loc[i,'id']] = train.loc[i,'songs']\n","\n","word_with_songs =[] # word_book와 인덱스가 맞춰져있다. 단어의 순서에 맞게 단어가 출현했을떄 사용된 곡들의 목록들이 들어있다.\n","for i in word_book.index:\n","  word_with_songs.append([]) \n","  for k in word_book.loc[i,'playlist']: #해당단어가 출현한 플레이리스트의 목록을 가져와서\n","    for j in songs_dic.get(k):  # 해당 플레이리스트에 사용된 곡들을 word_with_songs 에 집어넣는다 \n","      word_with_songs[i].append(j) # word_book의 크기에 인덱스를 맞춰놓앗다\n","\n","tmp_song_list = [Counter(i) for i in word_with_songs] #모든곡들의 중복을제거하면서 출현한 숫자를 카운트한다.\n","songs_count_list = [] # 카운팅된 숫자를 담을 리스트\n","for i , v in enumerate(tmp_song_list):\n","  count = word_book.loc[i,'playlist_cnt']\n","  songs_count_list.append([])\n","  for key , value in v.most_common(): #카운팅된 곡들을 가장많은 오름차순으로 정렬한다\n","    songs_count_list[i].append((key , value , value/count)) #저장하는방식 곡아이디 , 곡이출현한횟수 , 해당단어에서 곡이 출현확률 \n","\n","word_book['songs_cnt_appear_percent'] = songs_count_list\n","\n","# 플레이리스트가 2번이상 사용된 단어만 사용하기 위해서 데이터프레임을 하나 더 생성하고 \n","# 곡을 뽑을 시에 단어가 인풋으로 들어오면 곡의 출현 퍼센트를 이용해서 곡을 선택시 해당 filter_up2_word_book 이 사용된다. \n","filter_up2_word_book = word_book[word_book['playlist_cnt'] > 1] \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sl4V9Y6TqjNs","colab_type":"code","colab":{}},"source":["# 규정이 바뀌었으니 트레인, 발리데이션, 테스트 데이터셋에서 사용된 모든 곡만 임베딩을 진행한다\n","# 임베딩하는 이유는 곡을 뽑을떄 해당 단어가 일정퍼센트(30)이상 출현 할떄 퍼센트곡을 가져와서 사용하는데 \n","# 퍼센트 곡과 유사한곡을 구한후 정답에 추가 시키기 위해 사용된다.\n","\n","mix_train_val_test = pd.concat([train, val, test] , ignore_index=True)\n","songs_for_embedding = []\n","for i in mix_train_val_test.index:\n","  songs_for_embedding.append([])\n","  for k in mix_train_val_test.loc[i,'songs']:\n","    songs_for_embedding[i].append(str(k))\n","\n","w2v_model = Word2Vec(songs_for_embedding, min_count = 2, size=100, window=150, sg=1 , workers = multiprocessing.cpu_count())\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MO4qey-wqkY8","colab_type":"code","colab":{}},"source":["# 태그는 배열로 저장되어있어서 태그컬럼을 스트링으로 만들어 주는 포문\n","for i in test.index:\n","  tmp_str = \"\"\n","  for k in test.loc[i,'tags']:\n","    tmp_str = tmp_str + \" \" + k\n","  test.loc[i,'tags'] = tmp_str\n","\n","# 발리데이션에 사용된 제목과 태그 가져오기\n","test['plylst_title'] = re_sub(test['plylst_title'])\n","test.loc[:, 'ply_token'] = test['plylst_title'].map(lambda x: get_token(x, tokenizer))\n","test['tags_token'] = re_sub(test['tags'])\n","test.loc[:, 'tags_token'] = test['tags'].map(lambda x: get_token(x, tokenizer))\n","using_pos = ['NNG','SL','NNP','XR']\n","\n","test['ply_token'] = test['ply_token'].map(lambda x: list(filter(lambda x: x[1] in using_pos, x)))\n","test['ply_token'] = test['ply_token'].map(lambda x: list(filter(lambda x: not(x[0]) in no_use_items, x)))\n","\n","test['tags_token'] = test['tags_token'].map(lambda x: list(filter(lambda x: x[1] in using_pos, x)))\n","test['tags_token'] = test['tags_token'].map(lambda x: list(filter(lambda x: not(x[0]) in no_use_items, x)))\n","test_token = test[['tags_token','ply_token']].values.tolist()\n","\n","test_tags_title_list = [v  for v in test_token]\n","test_token_items = [] \n","# 하나의 리스트에 test-플레이리스트의 태그랑 토큰된 제목이 들어가 있는 배열\n","# 곡을 추출 하는 부분에서 단어를 이용해서 word_book에서 단어를 꺼내와야 하기때문에 문제와 인덱스를 맞추어서 단어를 저장하고 사용한다.\n","for i , v in enumerate(test_tags_title_list):\n","  test_token_items.append([])\n","  for k in v:\n","    for j in k:\n","      if type(j) == str:\n","        test_token_items[i].append(j)\n","      else: # 데이터 타입이 튜플일 경우를 엘스\n","        test_token_items[i].append(j[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l1TTPxikqpDU","colab_type":"code","colab":{}},"source":["# 곡을 뽑아주는 코드블럭\n","# 1. 문제 플레이리스트에서 곡이 없는경우 (제목이 존재)에만 동작한다\n","# 2. test_token_items(태그 , 제목 의 형태소가 들어가 있어서) 해당 문제 id에 사용된 단어들을 포문으로 전달\n","# 2-1. 단어들이 filter_up2_word_book에서 곡이 출현확률이 30프로 이상이면 similary_song_list에 추가\n","# 2-2 30프로 이상 곡이 없을 수도 있으니 단어에 가장많이 사용된 곡들을 top_using_songs에 추가\n","# 3. top_using_songs이 곡들을 앞에서 30개만 남겨놓는다. 협업으로 곡을 넘겨줄떄 너무많으면 필요없기 떄문 \n","# 4. 2-1에서 추가된 곡이 있으면 w2v으로 임배딩한 곡들 중에서 30프로 이상곡과 유사한곡들을 가져와 similary_song_list에 추가하고 50곡만추림\n","# 5 출현확률 30프로이상 곡이 없는경우 즉 similary_song_list의 크기가 0인경우 top_using_songs을 similary_song_list에 추가\n","# 6. 예외처리용 코드 : 한번더 시밀러곡이 0이면 곡추가시켜줌\n","# 7. 수집한 곡중에서 문제 플레이리스트의 최종수정일보다 이전에 발매된 곡만 사용하기위해 필터링후 filter_issue_song에 추가\n","# 7-1. 예외처리용 코드 : filter_issue_song의 크기가 0이면 top_using_songs의 곡이라도 추가 시켜줌\n","# 8.중복된 곡을 제거하여 final_song_list에 추가 후 협업쪽으로 전달 \n","\n","answers = []\n","get_tag = [\"기분전환\" , \"발라드\" , \" 휴식\" , \"드라이브\" , \"감성\" , \"잔잔한\" , \"힐링\" , \"카페\" , \"힙합\" , \" 새벽\"]\n","\n","for n, q in tqdm(enumerate(test_json), total=len(test_json)):\n","  if len(q['songs']) == 0: # 곡이 존재하지 않는 경우\n","    similary_song_list = [] #단어에 따른 곡의 출현빈도가 특정수치 이상인 단어들을 모으는 배열\n","    top_using_songs = []  # 많이 사용된 단어 목록을 담아놓고 모자른곡 채워넣음\n","    similary_embedding_songs = [] #워드임베딩해놓은 곡중에서 유사한곡 가져오기\n","    filter_issue_song = [] #문제의 최종수정일보다 이후에있는 발매일을 가진 곡은 제외\n","    final_song_list = [] #곡 최종제출용 \n","    \n","    test_update_time = q['updt_date'] \n","\n","    for word in test_token_items[n]: \n","      # 발리데이션에서 제목(토큰)과 태그를 가지고있을떄 해당 단어들을 포함하고있는 플레이리스트에서 곡이 출현확률이 0.3이상인거만 가져온다.\n","      try: #  단어가 단어장 filter_word_book에 존재 하지 않을경우를 예외처리\n","        targeted_songs = flatten(filter_up2_word_book.loc[filter_up2_word_book[\"word\"] == word,'songs_cnt_appear_percent'])\n","        targeted_songs = [i[0] for i in targeted_songs if i[2] > 0.3]\n","        similary_song_list.extend(targeted_songs)\n","      except:\n","        pass\n","      top_using_songs.append(word_book[word_book[\"word\"]== word]['playlist'].tolist())\n","\n","    top_using_songs = flatten(top_using_songs) \n","    top_using_songs = flatten(top_using_songs)\n","    top_using_songs = [songs_dic.get(i) for i in top_using_songs]\n","    top_using_songs = flatten(top_using_songs) \n","    # 협업필터링에 사용될 30개만 넘겨주기로함\n","    top_using_songs = list(pd.value_counts(top_using_songs)[:30].index) \n","\n","    # filter_up2_word_book에서 단어당 30프로 이상 출현확률을 가지는 곡들의 목록이 존재 하는 경우 곡만 w2v으로 임베딩한 곡과\n","    # 30프로 이상 출현곡의 유사한 곡을 더 가져옴 \n","    \n","    if len(similary_song_list) > 0:\n","      for i in similary_song_list:\n","        try: #단일곡 나오면 임배딩안되있어서 에러처리\n","          similary_embedding_songs.append(w2v_model.wv.most_similar(str(i)))\n","        except:\n","          pass\n","      similary_embedding_songs = flatten(similary_embedding_songs)\n","      similary_embedding_songs = [int(i[0]) for i in similary_embedding_songs]\n","      similary_song_list.extend(similary_embedding_songs) #임배딩으로 가져온 곡도 추가시켜줌 \n","      if len(similary_song_list) > 50: # 협업에 너무많이 50곡만 전달하기로 했으니 50곡이상일 경우 앞에서부터 곡을 잘라냄\n","        similary_song_list = similary_song_list[:50] \n","\n","    else:\n","      similary_song_list.extend(top_using_songs) #퍼센트 30프로 이상 곡이없으면 중복곡 30개를 넣어준다.\n","\n","    if len(similary_song_list) == 0: #예외처리용 코드 : 한번더 시밀러곡이 0이면 곡추가시켜줌\n","      similary_song_list.extend(top_using_songs)\n","\n","    for i in similary_song_list: \n","      # 수집한 곡중에서 문제 플레이리스트의 최종수정일보다 이전에 발매된 곡만 사용하기위해 필터링하는코드\n","      song_date = song_meta[song_meta[\"id\"]==i][\"issue_date\"].values[0]\n","      test_date = int(test_update_time.split(\"-\")[0]+test_update_time.split(\"-\")[1]+test_update_time.split(\"-\")[2].split(\" \")[0])\n","      if test_date-song_date>0:\n","        filter_issue_song.append(i)\n","\n","    # 발생하진 않을거 같은데 최종제출에 혹시 몰라서 아래 코드블럭은 예외처리를 위해 해놓음\n","    # 발매일 필터링으로 인한 제출용 곡이 하나도 없을 경우에는 탑 레이팅곡이라도 채워서 협업필터링에 전달한다.\n","    if len(filter_issue_song) == 0:\n","      filter_issue_song.extend(top_using_songs)\n","\n","    for v in filter_issue_song: #중복된 곡을 제거하여 final_song_list에 추가 하는 코드\n","      if v not in final_song_list:\n","        final_song_list.append(v)\n","\n","    final_song_list = final_song_list[:100]\n","\n","    answers.append({\"id\": q['id'], \"songs\": final_song_list, \"tags\": get_tag}) \n","  else:\n","    answers.append({\"id\": q['id'],\"songs\": most_results[n]['songs'],\"tags\": get_tag})\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qwPwAK9Gqq5p","colab_type":"code","colab":{}},"source":["\n","# ★★ 경로 수정 필요 ★★\n","write_json(answers, 'nosong_result.json')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ulmfa5olQTGT","colab_type":"text"},"source":["## 코사인 유사도를 통한 곡을 뽑는 모델"]},{"cell_type":"markdown","metadata":{"id":"31FoyaKeWWjh","colab_type":"text"},"source":["### 곡정보가 없는 부분,있는부분 두가지로 나누어 가져옴"]},{"cell_type":"code","metadata":{"id":"gMvW_EI9WU8H","colab_type":"code","colab":{}},"source":["##실제제출용은 TEST.JSON이기때문에 바꿈\n","val =test\n","\n","no_song_val = pd.read_json(\"arena_data/nosong_result.json\")\n","no_song_val = no_song_val[no_song_val[\"id\"].isin(val[val[\"songs\"].map(len)==0][\"id\"].values)]\n","has_song_val = val[val[\"songs\"].map(len)>0]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xzH-ql8oUOyF","colab_type":"code","colab":{}},"source":["def mapping_function(data, col1, col2): \n","    # 플레이리스트 아이디(col1)와 수록곡(col2) 추출\n","    plylst_song_map = data[[col1, col2]]\n","\n","    # unnest col2\n","    plylst_song_map_unnest = np.dstack(\n","        (\n","            np.repeat(plylst_song_map[col1].values, list(map(len, plylst_song_map[col2]))), \n","            np.concatenate(plylst_song_map[col2].values)\n","        )\n","    )\n","\n","    # unnested 데이터프레임 생성 : plylst_song_map\n","    plylst_song_map = pd.DataFrame(data = plylst_song_map_unnest[0], columns = plylst_song_map.columns)\n","    plylst_song_map[col1] = plylst_song_map[col1].astype(str)\n","    plylst_song_map[col2] = plylst_song_map[col2].astype(str)\n","\n","    # unnest 객체 제거\n","    del plylst_song_map_unnest\n","    return plylst_song_map"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BavcselKV53-","colab_type":"text"},"source":["### 실제 플레이리스트에만 나온 곡들을 정렬하고,dictionary를 만든후, 매트릭스를만든다 "]},{"cell_type":"code","metadata":{"id":"GHG-imTzUasn","colab_type":"code","colab":{}},"source":["playlst_song_map = mapping_function(train, 'id', 'songs')\n","agg = pd.DataFrame(playlst_song_map['songs'].value_counts()).reset_index()\n","agg.columns = ['곡', '플레이리스트내의 등장횟수']\n","song_meta['id'] = song_meta['id'].astype(str)\n","agg = agg.merge(song_meta[['id', 'artist_name_basket', 'song_name', 'song_gn_gnr_basket']], how='left', left_on='곡', right_on='id')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DJD_uv_7V91u","colab_type":"code","colab":{}},"source":["song_dic = {}\n","for i in tqdm(agg.index):\n","    id = agg.loc[i,\"곡\"]\n","    count = agg.loc[i,\"플레이리스트내의 등장횟수\"]\n","    song_dic[id] = count"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"83XL3p21WOMR","colab_type":"code","colab":{}},"source":["# 실제플레이리스트에만 나온 곡들만 가져오고 나오지 않은 곡들은 제외한다.\n","train[\"songs\"] = train[\"songs\"].map(lambda x: [ i  for i in  x if str(i) in song_dic])\n","val[\"songs\"] = val[\"songs\"].map(lambda x: [ i  for i in  x if str(i) in song_dic])\n","no_song_val[\"songs\"] = no_song_val[\"songs\"].map(lambda x: [ i  for i in  x if str(i) in song_dic])\n","has_song_val[\"songs\"] = has_song_val[\"songs\"].map(lambda x: [ i  for i in  x if str(i) in song_dic])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zEh6MfMzYlmC","colab_type":"code","colab":{}},"source":["# 학습플레이리스트에 들어있는 곡 개수를 따로 열 정보로 만들어준다.\n","train.loc[:,'num_songs'] = train['songs'].map(len)\n","# 활용할 플레이리스트 정보만 가져옴\n","plylst_train = train[['id','tags','plylst_title','songs','num_songs']]\n","\n","\n","#플레이리스트 -곡 매트릭스를 만들기위해 곡 id를 재 인덱싱한다\n","song_id_idx_dic = {}\n","song_idx_id_dic = {}\n","for k,i in enumerate(song_dic):\n","    song_id_idx_dic[str(k)] = i\n","    song_idx_id_dic[str(i)] = k\n","\n","#플레이리스트데이터들에 재인덱싱 곡id를 넣어준다\n","plylst_train['songs'] = plylst_train['songs'].map(lambda x:[song_idx_id_dic[str(i)] for i in x] )\n","val['songs'] = val['songs'].map(lambda x:[song_idx_id_dic[str(i)] for i in x] )\n","has_song_val['songs'] = has_song_val['songs'].map(lambda x:[song_idx_id_dic[str(i)] for i in x] )\n","no_song_val['songs'] = no_song_val['songs'].map(lambda x:[song_idx_id_dic[str(i)] for i in x] )\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0lw8rfUCYnzF","colab_type":"code","colab":{}},"source":["# 총 몇개의 곡이 들어있는지 저장 --> 매트릭스 data 정보\n","n_songs = sum(song_dic.values())\n","\n","# 총 몇개의 플레이리스트가 있는지 저장 --> 매트릭스 row 정보\n","n_plylst = len(plylst_train)\n","\n","# 곡 메타 정보에서 총 몇개의 곡이 들어있는지 저장 --> 매트릭스 col 정보\n","n_songs_all = len(song_dic)\n","\n","# 매트릭스에 들어갈 값 추출 (data)\n","data = np.repeat(1, n_songs)\n","\n","# row_position, col_position (data 값이 들어갈 position)\n","row_pos = np.repeat(range(n_plylst), plylst_train['num_songs'])\n","col_pos = [song for songs in plylst_train['songs'] for song in songs]\n","\n","# 플레이 리스트 별로 어떤 곡이 들어가 있는지 표현해주는 행렬 생성\n","plylst_song_matrix = spr.csr_matrix((data, (row_pos, col_pos)), shape=(n_plylst, n_songs_all))\n","### 필요한 function\n","print(plylst_song_matrix.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ICAd4ZyGcNnQ","colab_type":"text"},"source":["### 곡이 있을때 코사인유사도를 구하는부분"]},{"cell_type":"code","metadata":{"id":"s9y7LZPrb4M5","colab_type":"code","colab":{}},"source":["# 문제 플레이리스트 하나에 대한 매트릭스 만들어주는 함수\n","def has_song_plylst_to_matrix(val_song_list, n_songs_all):\n","    # 플레이리스트가 가지고 있는 곡의 수\n","    n_songs = len(val_song_list)\n","    \n","    # 매트릭스에 들어갈 값 추출 (data)\n","    data = np.repeat(1, n_songs)\n","    \n","    # row_position, col_position (data 값이 들어갈 position)\n","    row_pos = np.repeat(0, n_songs)\n","    col_pos = [song for song in val_song_list]\n","    \n","    # 플레이 리스트 별로 어떤 곡이 들어가 있는지 표현해주는 행렬 생성\n","    val_song_matrix = spr.csr_matrix((data, (row_pos, col_pos)), shape=(1, n_songs_all))\n","    \n","    return val_song_matrix\n","\n","# 플레이리스트 유사도 구해서 가장 비슷한 플레이리스트 뽑아오는 함수\n","def has_song_get_similar_plylsts_dict(val_song_matrix, plylst_song_matrix, top_n=10):\n","    \n","    # 플레이 리스트 별 유사도 구함\n","    plylst_sim = cosine_similarity(val_song_matrix, plylst_song_matrix)\n","    \n","    # 유사도가 높은 순으로 플레이리스트 index 정보 가져옴\n","    plylst_sim_sorted_ind = plylst_sim.argsort()[:, ::-1]\n","    similar_indexes = plylst_sim_sorted_ind[0, :(top_n)]\n","    \n","    # key : index, value : 유사도\n","    similar_plylst_dict = {x: round(plylst_sim[0,x],3) for x in similar_indexes if round(plylst_sim[0,x],3)}\n","    \n","    return similar_plylst_dict\n","\n","# 결과로 제출할 곡 뽑는 함수 listing\n","def has_song_get_result_songs(val_song_list,real_val_song_list, similar_plylst_dict, plylst_song_matrix):\n","    \n","    # 매트릭스에서 플레이리스트의 인덱스값을 의미하는 key값을 담을 리스트\n","    key_list = [key for key in similar_plylst_dict if len(similar_plylst_dict) > 0]\n","    \n","    # 키 리스트에 해당하는 행의 매트릭스 정보를 표로 만듬\n","    df =pd.DataFrame(plylst_song_matrix[key_list].todense())\n","    \n","    \n","    ##오래걸리는부분\n","    # 인덱스값과 키값을 차례로 뽑아서\n","    for i,key in enumerate(similar_plylst_dict):\n","        # 유사도를 뽑음\n","        \n","        value = similar_plylst_dict[key]\n","        \n","        # 해당 플레이리스트 행에 유사도를 곱함 (곡 별로 가중치를 주는 것)\n","        df[i] = df[i] * value\n","        \n","    # 열별로 유사도 전부 더해서 높은 순으로 후보 곡을 200개 뽑음\n","    cend_song = df.sum().sort_values(ascending=False)[:200]\n","    \n","    ##### 곡id를 원래대로바꿈\n","    \n","        \n","    \n","    # 결과로 담을 최종 곡 리스트\n","    result_song = []\n","    \n","    # 원래 가지고 있는 곡 제외하고 100개를 담음\n","    for index in cend_song.index:\n","        \n","        if index not in real_val_song_list:\n","            if index not in val_song_list:\n","                result_song.append(index)\n","        if len(result_song) >= 100:\n","            break\n","            #원래 곡을 바궈주는 과정\n","    result_song = [song_id_idx_dic[str(i)] for i in result_song]\n","    if len(result_song) < 100:\n","        # 협업필터링만으로 곡이 전부 안채워졌을 경우 인기곡 담는다.\n","        require = 100 - len(result_song)\n","        \n","        song_list_added_cend = [song for song in song_mp if song not in result_song]\n","        song_list_added = [song for song in song_list_added_cend if song not in val_song_list]\n","        \n","        result_song = result_song + song_list_added[:require]\n","        if len(result_song) != 100:\n","            print('코드 다시봐라')\n","        \n","    return result_song"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zbud9rTJc43M","colab_type":"code","colab":{}},"source":["val_plylst_id_dict = {x:y for (x,y) in enumerate(has_song_val['id'])}\n","val_plylst_songs_dict = {x:y for (x,y) in enumerate(has_song_val['songs'])}\n","real_val_plylst_songs_dict={x:y for (x,y) in enumerate(val['songs'])}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t6Qvc5bIc5nb","colab_type":"code","colab":{}},"source":["res = []\n","result_dict = {}\n","n_get_plylst_all = 0\n","n_val_plylst_all = len(val)\n","\n","for index in tqdm(range(len(val))):\n","    \n","    # 문제 플레이리스트 id 가져옴\n","    val_plylst_id = val_plylst_id_dict[index]\n","    # 그 플레이리스트가 가지고 있는 곡정보 가져옴\n","    val_already_songs = val_plylst_songs_dict[index]\n","    real_val_already_songs = real_val_plylst_songs_dict[index]\n","    # 문제 플레이리스트로 매트릭스 생성\n","    val_song_matrix = has_song_plylst_to_matrix(val_already_songs, n_songs_all)\n","    # 문제 플레이리스트와 가장 비슷한 플레이리스트들을 딕셔너리 형태로 가져옴\n","    similar_plylst_dict = has_song_get_similar_plylsts_dict(val_song_matrix, plylst_song_matrix)\n","    # 가져온 플레이리스트의 개수\n","    get_plylist_count = len(similar_plylst_dict)\n","    # 가져온 플레이리스트 개수 총 합 업데이트\n","    n_get_plylst_all = n_get_plylst_all + get_plylist_count\n","    # 문제 플레이리스트 id 별로 가져온 플레이리스트 개수 저장\n","    result_dict[val_plylst_id] = get_plylist_count\n","    # 곡 뽑아옴\n","    result_song = has_song_get_result_songs(val_already_songs,real_val_already_songs, similar_plylst_dict, plylst_song_matrix)\n","    # 결과에 추가\n","    res.append({\n","        \"id\": val_plylst_id,\n","        \"songs\": result_song,\n","        \"tags\": [\"기분전환\",\"발라드\",\"감성\",\"휴식\",\"드라이브\",\"힐링\",\"새벽\",\"잔잔한\",\"사랑\",\"카페\"]\n","    })\n","\n","print('----------------------------------------------')\n","print('평균적으로 가져오는 플레이리스트 개수 : ', n_get_plylst_all /  n_val_plylst_all)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"andzAGwzrT4C","colab_type":"code","colab":{}},"source":["write_json(res, \"cosine_model_has_song.json\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ubftak9IcQr3","colab_type":"text"},"source":["### 곡이 없을때 코사인 유사도를 구하는 부분"]},{"cell_type":"code","metadata":{"id":"R0IyngcDbYF_","colab_type":"code","colab":{}},"source":["# 문제 플레이리스트 하나에 대한 매트릭스 만들어주는 함수\n","def no_song_plylst_to_matrix(val_song_list, n_songs_all):\n","    # 플레이리스트가 가지고 있는 곡의 수\n","    n_songs = len(val_song_list)\n","    \n","    # 매트릭스에 들어갈 값 추출 (data)\n","    data = np.repeat(1, n_songs)\n","    \n","    # row_position, col_position (data 값이 들어갈 position)\n","    row_pos = np.repeat(0, n_songs)\n","    col_pos = [song for song in val_song_list]\n","    \n","    # 플레이 리스트 별로 어떤 곡이 들어가 있는지 표현해주는 행렬 생성\n","    val_song_matrix = spr.csr_matrix((data, (row_pos, col_pos)), shape=(1, n_songs_all))\n","    \n","    return val_song_matrix\n","\n","# 플레이리스트 유사도 구해서 가장 비슷한 플레이리스트 뽑아오는 함수\n","def no_song_get_similar_plylsts_dict(val_song_matrix, plylst_song_matrix, top_n=10):\n","    \n","    # 플레이 리스트 별 유사도 구함\n","    plylst_sim = cosine_similarity(val_song_matrix, plylst_song_matrix)\n","    \n","    # 유사도가 높은 순으로 플레이리스트 index 정보 가져옴\n","    plylst_sim_sorted_ind = plylst_sim.argsort()[:, ::-1]\n","    similar_indexes = plylst_sim_sorted_ind[0, :(top_n)]\n","    \n","    # key : index, value : 유사도\n","    similar_plylst_dict = {x: round(plylst_sim[0,x],3) for x in similar_indexes if round(plylst_sim[0,x],3)}\n","    \n","    return similar_plylst_dict\n","\n","# 결과로 제출할 곡 뽑는 함수 listing\n","def no_song_get_result_songs(val_song_list,real_val_song_list, similar_plylst_dict, plylst_song_matrix):\n","    \n","    # 매트릭스에서 플레이리스트의 인덱스값을 의미하는 key값을 담을 리스트\n","    key_list = [key for key in similar_plylst_dict if len(similar_plylst_dict) > 0]\n","    \n","    # 키 리스트에 해당하는 행의 매트릭스 정보를 표로 만듬\n","    df =pd.DataFrame(plylst_song_matrix[key_list].todense())\n","    \n","    \n","    ##오래걸리는부분\n","    # 인덱스값과 키값을 차례로 뽑아서\n","    for i,key in enumerate(similar_plylst_dict):\n","        # 유사도를 뽑음\n","        \n","        value = similar_plylst_dict[key]\n","        \n","        # 해당 플레이리스트 행에 유사도를 곱함 (곡 별로 가중치를 주는 것)\n","        df[i] = df[i] * value\n","        \n","    # 열별로 유사도 전부 더해서 높은 순으로 후보 곡을 200개 뽑음\n","    cend_song = df.sum().sort_values(ascending=False)[:200]\n","    \n","    ##### 곡id를 원래대로바꿈\n","    \n","        \n","    ##곡없을때는 유사곡을 구한후 밀어넣기를 하고 , 곡있을때는 그냥 카운트를 함\n","\n","    # 결과로 담을 최종 곡 리스트\n","    result_song = val_song_list\n","    \n","    # 원래 가지고 있는 곡 제외하고 100개를 담음\n","    for index in cend_song.index:\n","        \n","        if index not in real_val_song_list:\n","            if index not in val_song_list:\n","                result_song.append(index)\n","        if len(result_song) >= 100:\n","            break\n","            #원래 곡을 바궈주는 과정\n","    result_song = [song_id_idx_dic[str(i)] for i in result_song]\n","    if len(result_song) < 100:\n","        # 협업필터링만으로 곡이 전부 안채워졌을 경우 인기곡 담는다.\n","        require = 100 - len(result_song)\n","        \n","        song_list_added_cend = [song for song in song_mp if song not in result_song]\n","        song_list_added = [song for song in song_list_added_cend if song not in val_song_list]\n","        \n","        result_song = result_song + song_list_added[:require]\n","        if len(result_song) != 100:\n","            print('코드 다시봐라')\n","        \n","    return result_song\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HiJu2aDEcXfX","colab_type":"code","colab":{}},"source":["val_plylst_id_dict = {x:y for (x,y) in enumerate(no_song_val['id'])}\n","val_plylst_songs_dict = {x:y for (x,y) in enumerate(no_song_val['songs'])}\n","real_val_plylst_songs_dict={x:y for (x,y) in enumerate(val['songs'])}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ybN0C2arckv4","colab_type":"code","colab":{}},"source":["res = []\n","result_dict = {}\n","n_get_plylst_all = 0\n","n_val_plylst_all = len(val)\n","\n","for index in tqdm(range(len(val))):\n","    \n","    # 문제 플레이리스트 id 가져옴\n","    val_plylst_id = val_plylst_id_dict[index]\n","    # 그 플레이리스트가 가지고 있는 곡정보 가져옴\n","    val_already_songs = val_plylst_songs_dict[index]\n","    real_val_already_songs = real_val_plylst_songs_dict[index]\n","    \n","    # 문제 플레이리스트로 매트릭스 생성\n","    val_song_matrix = no_song_plylst_to_matrix(val_already_songs, n_songs_all)\n","    # 문제 플레이리스트와 가장 비슷한 플레이리스트들을 딕셔너리 형태로 가져옴\n","    similar_plylst_dict = no_song_get_similar_plylsts_dict(val_song_matrix, plylst_song_matrix)\n","    # 가져온 플레이리스트의 개수\n","    get_plylist_count = len(similar_plylst_dict)\n","    # 가져온 플레이리스트 개수 총 합 업데이트\n","    \n","    \n","    n_get_plylst_all = n_get_plylst_all + get_plylist_count\n","    # 문제 플레이리스트 id 별로 가져온 플레이리스트 개수 저장\n","    result_dict[val_plylst_id] = get_plylist_count\n","    # 곡 뽑아옴\n","    result_song = no_song_get_result_songs(val_already_songs,real_val_already_songs, similar_plylst_dict, plylst_song_matrix)\n","    # 결과에 추가\n","    res.append({\n","        \"id\": val_plylst_id,\n","        \"songs\": result_song,\n","        \"tags\": [\"기분전환\",\"발라드\",\"감성\",\"휴식\",\"드라이브\",\"힐링\",\"새벽\",\"잔잔한\",\"사랑\",\"카페\"]\n","    })\n","\n","print('----------------------------------------------')\n","print('평균적으로 가져오는 플레이리스트 개수 : ', n_get_plylst_all /  n_val_plylst_all)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KOzJ5JQRrs8b","colab_type":"code","colab":{}},"source":["write_json(res, \"cosine_model_no_song.json\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TXJB6VBhehAN","colab_type":"text"},"source":["## 기준에 맞춰 나온 모델들을 합치는 부분"]},{"cell_type":"code","metadata":{"id":"xsrOPyCPe4tY","colab_type":"code","colab":{}},"source":["##주 모델 코사인유사도\n","cosine_model_has_song = pd.read_json(\"arena_data/cosine_model_has_song.json\")\n","##곡없을때 뽑는 부분\n","cosine_model_no_song = pd.read_json(\"arena_data/cosine_model_no_song.json\")\n","##유사도 0.15이하\n","embedding_model_has_song = pd.read_json(\"arena_data/embedding_model_has_song.json\")\n","embedding_model_tag = pd.read_json(\"arena_data/embedding_model_tag.json\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QlH3oMR2eko1","colab_type":"text"},"source":["### 임베딩으로 뽑은 곡+코사인유사도로 뽑은 곡 많이 겹친 곡 순으로 다시 재정렬 한후 가져온다"]},{"cell_type":"code","metadata":{"id":"g42vcCf2e2UX","colab_type":"code","colab":{}},"source":["## 모델 1과 2의 곡들을 비교하여 겹치는곡을 우선으로 병합하는 부분\n","res = []\n","\n","for id in tqdm(cosine_model_has_song[\"id\"]):\n","    #id에 맞는 리스트를 가져온다.\n","    mo1 = cosine_model_has_song[cosine_model_has_song[\"id\"]==id]\n","    mo2 = embedding_model_has_song[embedding_model_has_song[\"id\"]==id]\n","    ## 높은 ndcg를 기록하는 모델을 기준으로 병합을 함 (현재 model1이 높은 ndcg보유)\n","    merged_list = mo1.merge(mo2, on=\"id\")\n","    \n","    \n","    \n","    #각각의 곡 리스트 합치기\n","    merged_song = merged_list[\"songs_x\"]+merged_list[\"songs_y\"]   \n","    songs_counter = Counter([tg for tgs in merged_song for tg in tgs])\n","    songs = []\n","    ## 합친 곡중 많이 겹친곡 순으로 100개 가져오기\n","    song_dict = {x: songs_counter[x] for x in songs_counter.most_common(100)}\n","    for i in song_dict:\n","        songs.append(i[0])\n","\n","\n","        \n","    #태그는 모델2의 태그만 가져오기때문에 X만 가져오면 됨 \n","    tags = merged_list[\"tags_y\"]\n","    tag = []\n","    tags_counter = Counter([tg for tgs in tags for tg in tgs])\n","    for k in tags_counter:\n","        tag.append(k)\n","        \n","    res.append({\n","            \"id\": id,\n","                \"songs\": songs,\n","                \"tags\": tag\n","         })\n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K5Ma2vQQe2RC","colab_type":"code","colab":{}},"source":["##합쳐진 18000개의 VAL 저장\n","write_json(res, \"hassong.json\")\n","has_song_model = pd.read_json(\"arena_data/hassong.json\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3u4IWWvFfOY8","colab_type":"code","colab":{}},"source":["list_has_song =  val[val[\"songs\"].map(len)>0].values\n","list_not_song =  val[val[\"songs\"].map(len)==0][\"id\"].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pac_XkkZfyfd","colab_type":"code","colab":{}},"source":["res = []\n","for idx in tqdm(list_has_song):\n","    songs = has_song_model[has_song_model[\"id\"]==idx][\"songs\"].values[0]\n","    tag = embedding_model_tag[embedding_model_tag[\"id\"]==idx][\"tags\"].values[0]\n","    \n","    res.append({\n","            \"id\": idx,\n","                \"songs\": songs,\n","                \"tags\": tag\n","         })\n","for idx in tqdm(list_not_song):\n","    songs = cosine_model_no_song[cosine_model_no_song[\"id\"]==idx][\"songs\"].values[0]\n","    tag = embedding_model_tag[embedding_model_tag[\"id\"]==idx][\"tags\"].values[0]\n","        \n","    res.append({\n","            \"id\": idx,\n","                \"songs\": songs,\n","                \"tags\": tag\n","         })"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SsBfl5P1gR6-","colab_type":"code","colab":{}},"source":["len(res)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"58y1VyF4gPOr","colab_type":"code","colab":{}},"source":["\n","write_json(res, \"final_results.json\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P-R-piXgEWSl","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}