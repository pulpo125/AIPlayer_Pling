{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 5위 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoEncoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=False)\n",
      "    (1): Linear(in_features=24666, out_features=450, bias=True)\n",
      "    (2): BatchNorm1d(450, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=450, out_features=24666, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Dropout-1                [-1, 24666]               0\n",
      "            Linear-2                  [-1, 450]      11,100,150\n",
      "       BatchNorm1d-3                  [-1, 450]             900\n",
      "         LeakyReLU-4                  [-1, 450]               0\n",
      "            Linear-5                [-1, 24666]      11,124,366\n",
      "           Sigmoid-6                [-1, 24666]               0\n",
      "================================================================\n",
      "Total params: 22,225,416\n",
      "Trainable params: 22,225,416\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.09\n",
      "Forward/backward pass size (MB): 0.57\n",
      "Params size (MB): 84.78\n",
      "Estimated Total Size (MB): 85.45\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out, dropout):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        encoder_layer = nn.Linear(D_in, H, bias=True)\n",
    "        decoder_layer = nn.Linear(H, D_out, bias=True)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(encoder_layer.weight)\n",
    "        torch.nn.init.xavier_uniform_(decoder_layer.weight)\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "                        nn.Dropout(dropout),\n",
    "                        encoder_layer,\n",
    "                        nn.BatchNorm1d(H),\n",
    "                        nn.LeakyReLU())\n",
    "        self.decoder = nn.Sequential(\n",
    "                        decoder_layer,\n",
    "                        nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_encoder = self.encoder(x)\n",
    "        out_decoder = self.decoder(out_encoder)\n",
    "        return out_decoder\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "D_in = 24666\n",
    "D_out = 24666\n",
    "H = 450\n",
    "dropout = 0.2\n",
    "model = AutoEncoder(D_in, H, D_out, dropout)\n",
    "\n",
    "# 모델 구조 출력\n",
    "print(model)\n",
    "\n",
    "from torchsummary import summary\n",
    "summary(model, (24666,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoEncoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=False)\n",
      "    (1): Linear(in_features=24666, out_features=64, bias=True)\n",
      "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=24666, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Dropout-1                [-1, 24666]               0\n",
      "            Linear-2                   [-1, 64]       1,578,688\n",
      "       BatchNorm1d-3                   [-1, 64]             128\n",
      "         LeakyReLU-4                   [-1, 64]               0\n",
      "            Linear-5                [-1, 24666]       1,603,290\n",
      "           Sigmoid-6                [-1, 24666]               0\n",
      "================================================================\n",
      "Total params: 3,182,106\n",
      "Trainable params: 3,182,106\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.09\n",
      "Forward/backward pass size (MB): 0.57\n",
      "Params size (MB): 12.14\n",
      "Estimated Total Size (MB): 12.80\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out, dropout):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        encoder_layer = nn.Linear(D_in, H, bias=True)\n",
    "        decoder_layer = nn.Linear(H, D_out, bias=True)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(encoder_layer.weight)\n",
    "        torch.nn.init.xavier_uniform_(decoder_layer.weight)\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "                        nn.Dropout(dropout),\n",
    "                        encoder_layer,\n",
    "                        nn.BatchNorm1d(H),\n",
    "                        nn.LeakyReLU())\n",
    "        self.decoder = nn.Sequential(\n",
    "                        decoder_layer,\n",
    "                        nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_encoder = self.encoder(x)\n",
    "        out_decoder = self.decoder(out_encoder)\n",
    "        return out_decoder\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "D_in = 24666\n",
    "D_out = 24666\n",
    "H = 64\n",
    "dropout = 0.2\n",
    "model = AutoEncoder(D_in, H, D_out, dropout)\n",
    "\n",
    "# 모델 구조 출력\n",
    "print(model)\n",
    "\n",
    "from torchsummary import summary\n",
    "summary(model, (24666,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tensorflow 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 24666)]           0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 24666)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 450)               11100150  \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 450)              1800      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 450)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 24666)             11124366  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,226,316\n",
      "Trainable params: 22,224,516\n",
      "Non-trainable params: 1,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# 모델 정의\n",
    "input_dim = 24666\n",
    "hidden_dim = 450\n",
    "dropout_rate = 0.2\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dropout(0.2)(input_layer)\n",
    "encoded = Dense(hidden_dim)(encoded)\n",
    "encoded = BatchNormalization(input_shape=(450,), trainable=False)(encoded)\n",
    "encoded = LeakyReLU(alpha=0.01)(encoded)\n",
    "\n",
    "decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "\n",
    "# 모델 요약 출력\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "trainable=False를 설정하는 이유는 배치 정규화 레이어를 학습 중에 고정하기 위해서입니다. 배치 정규화는 기본적으로 학습 중에 스케일 및 평균을 조정하면서 데이터의 정규화를 수행합니다. 그러나 때로는 이러한 스케일 및 평균을 고정하고 싶을 때가 있습니다.\n",
    "\n",
    "예를 들어, 사전 학습된 모델을 미세 조정하는 경우에는 사전 학습된 모델의 일부 레이어를 고정하고 싶을 수 있습니다. 이 경우 해당 레이어의 가중치와 통계 정보를 고정함으로써 미세 조정 중에 이 레이어가 크게 변경되지 않도록 할 수 있습니다.\n",
    "\n",
    "따라서 trainable=False를 설정하면 해당 레이어의 가중치 및 통계 정보를 학습 중에 업데이트하지 않고 고정할 수 있습니다. 이는 모델의 특정 부분을 고정하고 다른 부분만 학습시키려는 경우에 유용합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
