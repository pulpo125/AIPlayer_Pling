{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoder Basic: Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> model_1\n",
    "- input_dim = 24666 #len(mfl_col)\n",
    "- hidden_dim = 64\n",
    "- dropout = 0.2\n",
    "- BatchNormalization(input_shape=(hidden_dim,))\n",
    "- LeakyReLU(alpha=0.01)\n",
    "- epoch = 10\n",
    "- batch_size = 64\n",
    "- loss_function = binary_crossentropy\n",
    "- learning_rate = 0.0005\n",
    "- optimizer = adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import distutils.dir_util\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json write & load 함수 정의\n",
    "def write_json(data, fname):\n",
    "    def _conv(o):\n",
    "        if isinstance(o, (np.int64, np.int32)):\n",
    "            return int(o)\n",
    "        raise TypeError\n",
    "\n",
    "    parent = os.path.dirname(fname)\n",
    "    distutils.dir_util.mkpath(parent)\n",
    "    with io.open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json_str = json.dumps(data, ensure_ascii=False, default=_conv)\n",
    "        f.write(json_str)\n",
    "        \n",
    "def load_json(fname):\n",
    "    with open(fname, encoding='utf-8') as f:\n",
    "        json_obj = json.load(f)\n",
    "\n",
    "    return json_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45824, 24666)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../0_data/train_onehot.pkl', 'rb') as f:\n",
    "    train_onehot = pickle.load(f)\n",
    "\n",
    "train_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11456, 24666)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../0_data/test_onehot.pkl', 'rb') as f:\n",
    "    test_onehot = pickle.load(f)\n",
    "\n",
    "test_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24666"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../0_data/mfl_col.pkl', 'rb') as f:\n",
    "    mfl_col = pickle.load(f)\n",
    "\n",
    "len(mfl_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_len = 22798\n",
    "# song = 22798, tag = 1868"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11456 entries, 0 to 11455\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   tags          11456 non-null  object\n",
      " 1   id            11456 non-null  int64 \n",
      " 2   plylst_title  11456 non-null  object\n",
      " 3   songs         11456 non-null  object\n",
      " 4   like_cnt      11456 non-null  int64 \n",
      " 5   updt_date     11456 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 537.1+ KB\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_json('../0_data/test.json')\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import save_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 24666)]           0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24666)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                1578688   \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 64)               256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 24666)             1603290   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,182,234\n",
      "Trainable params: 3,182,106\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model_1\n",
    "input_dim = len(mfl_col)\n",
    "hidden_dim = 64\n",
    "dropout_rate = 0.2\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dropout(0.2)(input_layer)\n",
    "encoded = Dense(hidden_dim)(encoded)\n",
    "encoded = BatchNormalization(input_shape=(hidden_dim,))(encoded)\n",
    "encoded = LeakyReLU(alpha=0.01)(encoded)\n",
    "\n",
    "decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "model_1 = Model(inputs=input_layer, outputs=decoded)\n",
    "\n",
    "# 모델 요약 출력\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0005\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "model_1.compile(optimizer=optimizer,loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "573/573 [==============================] - 139s 239ms/step - loss: 0.1412 - val_loss: 0.0245\n",
      "Epoch 2/10\n",
      "573/573 [==============================] - 124s 217ms/step - loss: 0.0119 - val_loss: 0.0103\n",
      "Epoch 3/10\n",
      "573/573 [==============================] - 112s 195ms/step - loss: 0.0079 - val_loss: 0.0081\n",
      "Epoch 4/10\n",
      "573/573 [==============================] - 115s 200ms/step - loss: 0.0070 - val_loss: 0.0073\n",
      "Epoch 5/10\n",
      "573/573 [==============================] - 118s 205ms/step - loss: 0.0065 - val_loss: 0.0068\n",
      "Epoch 6/10\n",
      "573/573 [==============================] - 120s 209ms/step - loss: 0.0062 - val_loss: 0.0064\n",
      "Epoch 7/10\n",
      "573/573 [==============================] - 119s 207ms/step - loss: 0.0059 - val_loss: 0.0061\n",
      "Epoch 8/10\n",
      "573/573 [==============================] - 120s 209ms/step - loss: 0.0056 - val_loss: 0.0058\n",
      "Epoch 9/10\n",
      "573/573 [==============================] - 118s 206ms/step - loss: 0.0054 - val_loss: 0.0056\n",
      "Epoch 10/10\n",
      "573/573 [==============================] - 125s 218ms/step - loss: 0.0052 - val_loss: 0.0054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x162cd7c18d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.fit(train_onehot,train_onehot,epochs=10, batch_size=64, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.save('results/model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = tf.keras.models.load_model('results/model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358/358 [==============================] - 10s 26ms/step\n"
     ]
    }
   ],
   "source": [
    "predict_plist=model_1.predict(test_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_id = list(test['id'])\n",
    "col= mfl_col\n",
    "ori_song = col[:song_len]\n",
    "ori_tag = col[song_len:]\n",
    "\n",
    "song_predict = predict_plist[:,:song_len] # song output(추천곡)\n",
    "tag_predict = predict_plist[:,song_len:] # tag output(추천태그)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=[]\n",
    "n=0\n",
    "for i in df_id:\n",
    "    dic={}\n",
    "    dic['id']=i\n",
    "\n",
    "    plist_song=song_predict[n].argsort()[-100:] # predict한 song output 중 상위 100개\n",
    "    p_song=[]\n",
    "    for song in plist_song:\n",
    "        p_song.append(ori_song[song])\n",
    "    dic['songs']=p_song\n",
    "\n",
    "    plist_tag=tag_predict[n].argsort()[-10:] # predict한 tag output 중 상위 10개\n",
    "    p_tag=[]\n",
    "    for tag in plist_tag:\n",
    "        p_tag.append(ori_tag[tag])\n",
    "    dic['tags']=p_tag\n",
    "    n+=1\n",
    "    result.append(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json(result,'results/result_model_1.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArenaEvaluator:\n",
    "    def _idcg(self, l):\n",
    "        return sum((1.0 / np.log(i + 2) for i in range(l)))\n",
    "\n",
    "    def __init__(self):\n",
    "        self._idcgs = [self._idcg(i) for i in range(101)]\n",
    "\n",
    "    def _ndcg(self, gt, rec):\n",
    "        dcg = 0.0\n",
    "        for i, r in enumerate(rec):\n",
    "            if r in gt:\n",
    "                dcg += 1.0 / np.log(i + 2)\n",
    "        if len(gt)>100:\n",
    "            gt = gt[:100]\n",
    "        return dcg / self._idcgs[len(gt)]\n",
    "\n",
    "    def _eval(self, gt_fname, rec_fname):\n",
    "        gt_playlists = load_json(gt_fname)\n",
    "        gt_dict = {g[\"id\"]: g for g in gt_playlists}\n",
    "        rec_playlists = load_json(rec_fname)\n",
    "        gt_ids = set([g[\"id\"] for g in gt_playlists])\n",
    "        rec_ids = set([r[\"id\"] for r in rec_playlists])\n",
    "        if gt_ids != rec_ids:\n",
    "            raise Exception(\"결과의 플레이리스트 수가 올바르지 않습니다.\")\n",
    "\n",
    "        rec_song_counts = [len(p[\"songs\"]) for p in rec_playlists]\n",
    "        rec_tag_counts = [len(p[\"tags\"]) for p in rec_playlists]\n",
    "        if set(rec_song_counts) != set([100]):\n",
    "            raise Exception(\"추천 곡 결과의 개수가 맞지 않습니다.\")\n",
    "\n",
    "        if set(rec_tag_counts) != set([10]):\n",
    "            raise Exception(\"추천 태그 결과의 개수가 맞지 않습니다.\")\n",
    "\n",
    "        rec_unique_song_counts = [len(set(p[\"songs\"])) for p in rec_playlists]\n",
    "        rec_unique_tag_counts = [len(set(p[\"tags\"])) for p in rec_playlists]\n",
    "\n",
    "        if set(rec_unique_song_counts) != set([100]):\n",
    "            raise Exception(\"한 플레이리스트에 중복된 곡 추천은 허용되지 않습니다.\")\n",
    "\n",
    "        if set(rec_unique_tag_counts) != set([10]):\n",
    "            raise Exception(\"한 플레이리스트에 중복된 태그 추천은 허용되지 않습니다.\")\n",
    "\n",
    "        music_ndcg = 0.0\n",
    "        tag_ndcg = 0.0\n",
    "\n",
    "        for rec in rec_playlists:\n",
    "            gt = gt_dict[rec[\"id\"]]\n",
    "            music_ndcg += self._ndcg(gt[\"songs\"], rec[\"songs\"][:100])\n",
    "            tag_ndcg += self._ndcg(gt[\"tags\"], rec[\"tags\"][:10])\n",
    "\n",
    "        music_ndcg = music_ndcg / len(rec_playlists)\n",
    "        tag_ndcg = tag_ndcg / len(rec_playlists)\n",
    "        score = music_ndcg * 0.85 + tag_ndcg * 0.15\n",
    "\n",
    "        return music_ndcg, tag_ndcg, score\n",
    "\n",
    "    def evaluate_with_save(self, gt_fname, rec_fname, model_file_path, default_file_path):\n",
    "        # try:\n",
    "        music_ndcg, tag_ndcg, score = self._eval(gt_fname, rec_fname)\n",
    "        with open(f'{default_file_path}/results.txt','a') as f:\n",
    "            f.write(model_file_path)\n",
    "            f.write(f\"\\nMusic nDCG: {music_ndcg:.6}\\n\")\n",
    "            f.write(f\"Tag nDCG: {tag_ndcg:.6}\\n\")\n",
    "            f.write(f\"Score: {score:.6}\\n\\n\")\n",
    "            print(f\"Music nDCG: {music_ndcg:.6}\")\n",
    "            print(f\"Tag nDCG: {tag_ndcg:.6}\")\n",
    "            print(f\"Score: {score:.6}\")\n",
    "        # except Exception as e:\n",
    "        #     print(e)\n",
    "\n",
    "    def evaluate(self, gt_fname, rec_fname):\n",
    "        # try:\n",
    "        music_ndcg, tag_ndcg, score = self._eval(gt_fname, rec_fname)\n",
    "        print(f\"Music nDCG: {music_ndcg:.6}\")\n",
    "        print(f\"Tag nDCG: {tag_ndcg:.6}\")\n",
    "        print(f\"Score: {score:.6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Music nDCG: 0.126488\n",
      "Tag nDCG: 0.328678\n",
      "Score: 0.156816\n"
     ]
    }
   ],
   "source": [
    "gt_fname = '../0_data/test.json'\n",
    "rec_fname = 'results/result_model_1.json'\n",
    "arena_evaluator = ArenaEvaluator()\n",
    "arena_evaluator.evaluate(gt_fname, rec_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
