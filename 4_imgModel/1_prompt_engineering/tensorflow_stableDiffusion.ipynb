{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. pip & import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: googletrans==4.0.0-rc1 in c:\\mockup\\env\\lib\\site-packages (4.0.0rc1)\n",
      "Requirement already satisfied: httpx==0.13.3 in c:\\mockup\\env\\lib\\site-packages (from googletrans==4.0.0-rc1) (0.13.3)\n",
      "Requirement already satisfied: certifi in c:\\mockup\\env\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2023.7.22)\n",
      "Requirement already satisfied: hstspreload in c:\\mockup\\env\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2023.1.1)\n",
      "Requirement already satisfied: sniffio in c:\\mockup\\env\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.0)\n",
      "Requirement already satisfied: chardet==3.* in c:\\mockup\\env\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
      "Requirement already satisfied: idna==2.* in c:\\mockup\\env\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in c:\\mockup\\env\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
      "Requirement already satisfied: httpcore==0.9.* in c:\\mockup\\env\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in c:\\mockup\\env\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n",
      "Requirement already satisfied: h2==3.* in c:\\mockup\\env\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in c:\\mockup\\env\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in c:\\mockup\\env\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n"
     ]
    }
   ],
   "source": [
    "pip install googletrans==4.0.0-rc1  # 번역\n",
    "pip install transformers # Pytorch StableDiffusion\n",
    "pip install diffusers # Pytorch StableDiffusion\n",
    "pip install matplotlib # 이미지 나타내기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 공통"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\k9942\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\k9942\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from googletrans import Translator\n",
    "\n",
    "nltk.download('punkt')  # 토큰화\n",
    "nltk.download('stopwords')  # 불용어 리스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pytorch StableDiffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\mockup\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch, logging\n",
    "\n",
    "## Imaging  library\n",
    "from PIL import Image\n",
    "from torchvision import transforms as tfms\n",
    "\n",
    "## Basic libraries\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "## Import the CLIP artifacts\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator=Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_title =  \"강가에서\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On the river'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_title_trans=translator.translate(new_title,dest='en').text\n",
    "new_title_trans # 영어로 번역"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = 'photograph, ' + new_title_trans + ', landscape, 8k uhd'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dark', 'alleyway']\n"
     ]
    }
   ],
   "source": [
    "stopwords=nltk.corpus.stopwords.words('english') # 영어의 불용어 단어 리스트 (불용어는 모두 소문자)\n",
    "\n",
    "tokens=nltk.word_tokenize(new_title_trans)  # 각 문장을 토큰화\n",
    "\n",
    "tokens=[t for t in tokens if t.lower() not in stopwords] # 토큰화한 각 문장에서 불용어 제거\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dark', 'alleyway', 'photograph', 'landscape', 'simple']\n"
     ]
    }
   ],
   "source": [
    "remove_prompts = ['good', 'song', 'listen', 'hear'] # '듣기 좋은 노래' 번역, 토큰화 결과 제거\n",
    "\n",
    "prompts = [item for item in tokens if item not in remove_prompts] # 기존의 토큰 리스트에서 삭제할 프롬프트 제거\n",
    "\n",
    "additional_prompts = ['photograph', 'landscape', 'simple'] # 스타일 프롬프트 추가\n",
    "prompts = prompts + additional_prompts\n",
    "print(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dark,alleyway,photograph,landscape,simple\n"
     ]
    }
   ],
   "source": [
    "input_prompts=','.join(prompts)\n",
    "print(input_prompts)  # input 형식에 맞춰 수정\n",
    "\n",
    "# 파이토치- []로 감싸야 함\n",
    "# 케라스, 텐서플로우- 그대로 넣어도 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 이미지 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 토크나이저 및 인코더 초기화\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "## VAE 초기화\n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "## 스케줄러 초기화 및 샘플링 스텝 수 설정\n",
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "scheduler.set_timesteps(50)\n",
    "\n",
    "## U-Net 모델 초기화\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "## 도우미 함수\n",
    "def load_image(p):\n",
    "    '''\n",
    "    지정된 경로에서 이미지를 불러오는 함수\n",
    "    '''\n",
    "    return Image.open(p).convert('RGB').resize((512,512))\n",
    "\n",
    "def pil_to_latents(image):\n",
    "    '''\n",
    "    이미지를 latent(잠재)으로 변환하는 함수\n",
    "    '''\n",
    "    init_image = tfms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0\n",
    "    init_image = init_image.to(device=\"cuda\", dtype=torch.float16)\n",
    "    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215\n",
    "    return init_latent_dist\n",
    "\n",
    "def latents_to_pil(latents):\n",
    "    '''\n",
    "    latent(잠재)을 이미지로 변환하는 함수\n",
    "    '''\n",
    "    latents = (1 / 0.18215) * latents\n",
    "    with torch.no_grad():\n",
    "        image = vae.decode(latents).sample\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image * 255).round().astype(\"uint8\")\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "    return pil_images\n",
    "\n",
    "def text_enc(prompts, maxlen=None):\n",
    "    '''\n",
    "    텍스트 프롬프트를 임베딩으로 변환하는 함수\n",
    "    '''\n",
    "    if maxlen is None: maxlen = tokenizer.model_max_length\n",
    "    inp = tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\")\n",
    "    return text_encoder(inp.input_ids.to(\"cuda\"))[0].half()\n",
    "\n",
    "def prompt_2_img(prompts, neg_prompts=None, g=7.5, seed=100, steps=70, dim=512, save_int=False):\n",
    "    \"\"\"\n",
    "    이미지로 프롬프트를 변환하는 확산 프로세스\n",
    "    \"\"\"\n",
    "\n",
    "    # 배치 크기 정의\n",
    "    bs = len(prompts)\n",
    "\n",
    "    # 텍스트 프롬프트를 임베딩으로 변환\n",
    "    text = text_enc(prompts)\n",
    "\n",
    "    # 부정적인 프롬프트 조건 추가\n",
    "    if not neg_prompts: uncond =  text_enc([\"\"] * bs, text.shape[1])\n",
    "    # 무조건적인 프롬프트 추가, 생성 프로세스를 돕는 데 도움이 됨\n",
    "    else: uncond =  text_enc(neg_prompts, text.shape[1])\n",
    "    emb = torch.cat([uncond, text])\n",
    "\n",
    "    # 시드 설정\n",
    "    if seed: torch.manual_seed(seed)\n",
    "\n",
    "    # 무작위 노이즈 초기화\n",
    "    latents = torch.randn((bs, unet.in_channels, dim//8, dim//8))\n",
    "\n",
    "    # 스케줄러에서 스텝 수 설정\n",
    "    scheduler.set_timesteps(steps)\n",
    "\n",
    "    # 노이즈를 잠재 변수에 추가\n",
    "    latents = latents.to(\"cuda\").half() * scheduler.init_noise_sigma\n",
    "\n",
    "    # 정의된 스텝 수를 반복\n",
    "    for i,ts in enumerate(tqdm(scheduler.timesteps)):\n",
    "        # 입력 잠재 변수를 분산과 일치하도록 스케일링해야 함\n",
    "        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n",
    "\n",
    "        # U-Net을 사용하여 노이즈 잔여 값을 예측\n",
    "        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n",
    "\n",
    "        # 가이던스 수행\n",
    "        pred = u + g*(t-u)\n",
    "\n",
    "        # 잠재 변수 조건\n",
    "        latents = scheduler.step(pred, ts, latents).prev_sample\n",
    "\n",
    "        # 중간 이미지 저장\n",
    "        if save_int:\n",
    "            if not os.path.exists(f'./steps'): os.mkdir(f'./steps')\n",
    "            latents_to_pil(latents)[0].save(f'steps/{i:04}.jpeg')\n",
    "\n",
    "    # 잠재 표현을 반환하여 3x512x512 크기의 이미지 생성\n",
    "    return latents_to_pil(latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = prompt_2_img(prompts = [\"beach,landscape,photograph,simple\"]\n",
    "                     ,neg_prompts=[\"body, hair, face, eyes, nose, ears, lip, legs, arms, hands, feet, girl, boy, people\"]\n",
    "                     ,steps=50, save_int=False)[0]\n",
    "new_width, new_height = (1280, 720)  # 유튜브 썸네일 이미지 규격\n",
    "images = images.resize((new_width, new_height), Image.ANTIALIAS)\n",
    "plt.imshow(images)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 이미지 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장할 폴더 경로 설정\n",
    "output_folder = \"result\"\n",
    "\n",
    "# 폴더가 존재하지 않으면 생성\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# 생성된 이미지 파일의 이름\n",
    "output_file_name = f\"{new_title}.jpg\"\n",
    "\n",
    "# 이미지 파일의 전체 경로 설정\n",
    "output_file_path = os.path.join(output_folder, output_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 저장\n",
    "images.save(output_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
