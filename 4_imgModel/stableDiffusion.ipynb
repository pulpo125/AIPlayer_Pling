{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. pip & import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: googletrans==4.0.0-rc1 in c:\\mockup\\env\\lib\\site-packages (4.0.0rc1)\n",
      "Requirement already satisfied: httpx==0.13.3 in c:\\mockup\\env\\lib\\site-packages (from googletrans==4.0.0-rc1) (0.13.3)\n",
      "Requirement already satisfied: certifi in c:\\mockup\\env\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2023.7.22)\n",
      "Requirement already satisfied: hstspreload in c:\\mockup\\env\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2023.1.1)\n",
      "Requirement already satisfied: sniffio in c:\\mockup\\env\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.0)\n",
      "Requirement already satisfied: chardet==3.* in c:\\mockup\\env\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
      "Requirement already satisfied: idna==2.* in c:\\mockup\\env\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in c:\\mockup\\env\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
      "Requirement already satisfied: httpcore==0.9.* in c:\\mockup\\env\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in c:\\mockup\\env\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n",
      "Requirement already satisfied: h2==3.* in c:\\mockup\\env\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in c:\\mockup\\env\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in c:\\mockup\\env\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install googletrans==4.0.0-rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\mockup\\env\\lib\\site-packages (4.33.2)\n",
      "Requirement already satisfied: filelock in c:\\mockup\\env\\lib\\site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\mockup\\env\\lib\\site-packages (from transformers) (0.17.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\mockup\\env\\lib\\site-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\mockup\\env\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\mockup\\env\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\mockup\\env\\lib\\site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in c:\\mockup\\env\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\mockup\\env\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\mockup\\env\\lib\\site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\mockup\\env\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in c:\\mockup\\env\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\mockup\\env\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.8.0)\n",
      "Requirement already satisfied: colorama in c:\\mockup\\env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\mockup\\env\\lib\\site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\mockup\\env\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\mockup\\env\\lib\\site-packages (from requests->transformers) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\mockup\\env\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: diffusers in c:\\mockup\\env\\lib\\site-packages (0.21.4)\n",
      "Requirement already satisfied: Pillow in c:\\mockup\\env\\lib\\site-packages (from diffusers) (10.0.1)\n",
      "Requirement already satisfied: filelock in c:\\mockup\\env\\lib\\site-packages (from diffusers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.13.2 in c:\\mockup\\env\\lib\\site-packages (from diffusers) (0.17.2)\n",
      "Requirement already satisfied: importlib-metadata in c:\\mockup\\env\\lib\\site-packages (from diffusers) (6.8.0)\n",
      "Requirement already satisfied: numpy in c:\\mockup\\env\\lib\\site-packages (from diffusers) (1.26.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\mockup\\env\\lib\\site-packages (from diffusers) (2023.8.8)\n",
      "Requirement already satisfied: requests in c:\\mockup\\env\\lib\\site-packages (from diffusers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\mockup\\env\\lib\\site-packages (from diffusers) (0.3.3)\n",
      "Requirement already satisfied: fsspec in c:\\mockup\\env\\lib\\site-packages (from huggingface-hub>=0.13.2->diffusers) (2023.9.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\mockup\\env\\lib\\site-packages (from huggingface-hub>=0.13.2->diffusers) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\mockup\\env\\lib\\site-packages (from huggingface-hub>=0.13.2->diffusers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\mockup\\env\\lib\\site-packages (from huggingface-hub>=0.13.2->diffusers) (4.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\mockup\\env\\lib\\site-packages (from huggingface-hub>=0.13.2->diffusers) (23.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\mockup\\env\\lib\\site-packages (from importlib-metadata->diffusers) (3.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\mockup\\env\\lib\\site-packages (from requests->diffusers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\mockup\\env\\lib\\site-packages (from requests->diffusers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\mockup\\env\\lib\\site-packages (from requests->diffusers) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\mockup\\env\\lib\\site-packages (from requests->diffusers) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\mockup\\env\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.13.2->diffusers) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\mockup\\env\\lib\\site-packages (3.8.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\mockup\\env\\lib\\site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\mockup\\env\\lib\\site-packages (from matplotlib) (0.12.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\mockup\\env\\lib\\site-packages (from matplotlib) (4.43.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\mockup\\env\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\mockup\\env\\lib\\site-packages (from matplotlib) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\mockup\\env\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\mockup\\env\\lib\\site-packages (from matplotlib) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\mockup\\env\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\mockup\\env\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\mockup\\env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\mockup\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch, logging\n",
    "from PIL import Image\n",
    "from torchvision import transforms as tfms\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 문장 번역"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "translator=Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_title = '가을 산책할 때 듣기 좋은 노래'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A good song to listen to when walking in autumn'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_title_trans=translator.translate(new_title,dest='en').text\n",
    "new_title_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'song', 'listen', 'walking', 'autumn']\n"
     ]
    }
   ],
   "source": [
    "# 영어의 불용어 단어 리스트를 불러와 변수에 저장\n",
    "stopwords=nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# 각 문장을 토큰화\n",
    "tokens=nltk.word_tokenize(new_title_trans)\n",
    "\n",
    "# 토큰화한 각 문장에서 불용어를 제거한 결과 출력\n",
    "# 불용어가 모두 소문자이므로, 각 토큰을 소문자로 변환 후 비교해야 함\n",
    "tokens=[t for t in tokens if t.lower() not in stopwords]\n",
    "print(tokens)\n",
    "# nltk를 사용한 이유 명확하게 정리(<->konlpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_prompts = ['good', 'song', 'listen', 'hear']\n",
    "\n",
    "# 기존의 토큰 리스트에서 삭제할 프롬프트 제거\n",
    "prompts = [item for item in tokens if item not in remove_prompts]\n",
    "\n",
    "# 필수 프롬프트 추가\n",
    "additional_prompts = ['photograph, landscape, simple']\n",
    "prompts = prompts + additional_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 이미지 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "value cannot be converted to type at::Half without overflow",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\mockup\\4_imgModel\\stableDiffusion.ipynb 셀 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/mockup/4_imgModel/stableDiffusion.ipynb#X20sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m latents_to_pil(latents)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/mockup/4_imgModel/stableDiffusion.ipynb#X20sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m \u001b[39m# 이미지 생성 및 표시\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/mockup/4_imgModel/stableDiffusion.ipynb#X20sZmlsZQ%3D%3D?line=138'>139</a>\u001b[0m images \u001b[39m=\u001b[39m prompt_2_img(prompts\u001b[39m=\u001b[39;49mprompts, neg_prompts\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mbody, hair, face, eyes, nose, ears, lip, legs, arms, hands, feet, girl, boy, people\u001b[39;49m\u001b[39m\"\u001b[39;49m], steps\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, save_int\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/mockup/4_imgModel/stableDiffusion.ipynb#X20sZmlsZQ%3D%3D?line=140'>141</a>\u001b[0m \u001b[39m# 수정된 이미지 표시\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/mockup/4_imgModel/stableDiffusion.ipynb#X20sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow(images)\n",
      "\u001b[1;32mc:\\mockup\\4_imgModel\\stableDiffusion.ipynb 셀 15\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/mockup/4_imgModel/stableDiffusion.ipynb#X20sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/mockup/4_imgModel/stableDiffusion.ipynb#X20sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m \u001b[39mDiffusion process to convert prompt to image\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/mockup/4_imgModel/stableDiffusion.ipynb#X20sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/mockup/4_imgModel/stableDiffusion.ipynb#X20sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m \u001b[39m# input_ids를 float32로 변환\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/mockup/4_imgModel/stableDiffusion.ipynb#X20sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m input_ids \u001b[39m=\u001b[39m text_enc(prompts)\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/mockup/4_imgModel/stableDiffusion.ipynb#X20sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m \u001b[39m# 다음으로 이어지는 부분은 변경 없음\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/mockup/4_imgModel/stableDiffusion.ipynb#X20sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m bs \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(prompts)\n",
      "\u001b[1;32mc:\\mockup\\4_imgModel\\stableDiffusion.ipynb 셀 15\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/mockup/4_imgModel/stableDiffusion.ipynb#X20sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39mif\u001b[39;00m maxlen \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m: maxlen \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mmodel_max_length\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/mockup/4_imgModel/stableDiffusion.ipynb#X20sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m inp \u001b[39m=\u001b[39m tokenizer(prompts, padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m, max_length\u001b[39m=\u001b[39mmaxlen, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/mockup/4_imgModel/stableDiffusion.ipynb#X20sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39mreturn\u001b[39;00m text_encoder(inp\u001b[39m.\u001b[39;49minput_ids)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\mockup\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\mockup\\env\\lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:840\u001b[0m, in \u001b[0;36mCLIPTextModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    822\u001b[0m \u001b[39mReturns:\u001b[39;00m\n\u001b[0;32m    823\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    836\u001b[0m \u001b[39m>>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\u001b[39;00m\n\u001b[0;32m    837\u001b[0m \u001b[39m```\"\"\"\u001b[39;00m\n\u001b[0;32m    838\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m--> 840\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext_model(\n\u001b[0;32m    841\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m    842\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    843\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m    844\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    845\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    846\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    847\u001b[0m )\n",
      "File \u001b[1;32mc:\\mockup\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\mockup\\env\\lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:739\u001b[0m, in \u001b[0;36mCLIPTextTransformer.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    735\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(input_ids\u001b[39m=\u001b[39minput_ids, position_ids\u001b[39m=\u001b[39mposition_ids)\n\u001b[0;32m    737\u001b[0m \u001b[39m# CLIP's text model uses causal mask, prepare it here.\u001b[39;00m\n\u001b[0;32m    738\u001b[0m \u001b[39m# https://github.com/openai/CLIP/blob/cfcffb90e69f37bf2ff1e988237a0fbe41f33c04/clip/model.py#L324\u001b[39;00m\n\u001b[1;32m--> 739\u001b[0m causal_attention_mask \u001b[39m=\u001b[39m _make_causal_mask(input_shape, hidden_states\u001b[39m.\u001b[39;49mdtype, device\u001b[39m=\u001b[39;49mhidden_states\u001b[39m.\u001b[39;49mdevice)\n\u001b[0;32m    740\u001b[0m \u001b[39m# expand attention_mask\u001b[39;00m\n\u001b[0;32m    741\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    742\u001b[0m     \u001b[39m# [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\mockup\\env\\lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:686\u001b[0m, in \u001b[0;36m_make_causal_mask\u001b[1;34m(input_ids_shape, dtype, device, past_key_values_length)\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    683\u001b[0m \u001b[39mMake causal mask used for bi-directional self-attention.\u001b[39;00m\n\u001b[0;32m    684\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    685\u001b[0m bsz, tgt_len \u001b[39m=\u001b[39m input_ids_shape\n\u001b[1;32m--> 686\u001b[0m mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfull((tgt_len, tgt_len), torch\u001b[39m.\u001b[39;49mfinfo(dtype)\u001b[39m.\u001b[39;49mmin, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[0;32m    687\u001b[0m mask_cond \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(mask\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m    688\u001b[0m mask\u001b[39m.\u001b[39mmasked_fill_(mask_cond \u001b[39m<\u001b[39m (mask_cond \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mview(mask\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), \u001b[39m1\u001b[39m), \u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: value cannot be converted to type at::Half without overflow"
     ]
    }
   ],
   "source": [
    "import torch, logging\n",
    "\n",
    "## disable warnings\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "## Imaging  library\n",
    "from PIL import Image\n",
    "from torchvision import transforms as tfms\n",
    "\n",
    "## Basic libraries\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "## For video display\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "## Import the CLIP artifacts\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n",
    "\n",
    "## Initiating tokenizer and encoder.\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "# 모델을 'float32'로 변환\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(torch.float32)\n",
    "\n",
    "## Initiating the VAE\n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n",
    "# 모델을 'float32'로 변환\n",
    "vae = vae.to(torch.float32)\n",
    "\n",
    "## Initializing a scheduler and Setting number of sampling steps\n",
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "scheduler.set_timesteps(50)\n",
    "\n",
    "## Initializing the U-Net model\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")\n",
    "# 모델을 'float32'로 변환\n",
    "unet = unet.to(torch.float32)\n",
    "\n",
    "## Helper functions\n",
    "def load_image(p):\n",
    "    '''\n",
    "    Function to load images from a defined path\n",
    "    '''\n",
    "    return Image.open(p).convert('RGB').resize((512,512))\n",
    "\n",
    "def pil_to_latents(image):\n",
    "    '''\n",
    "    Function to convert image to latents\n",
    "    '''\n",
    "    init_image = tfms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0\n",
    "    init_image = init_image.to(device=\"cpu\", dtype=torch.float32)  # CPU로 이동 및 데이터 타입 변경\n",
    "    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215\n",
    "    return init_latent_dist\n",
    "\n",
    "def latents_to_pil(latents):\n",
    "    '''\n",
    "    Function to convert latents to images\n",
    "    '''\n",
    "    latents = (1 / 0.18215) * latents\n",
    "    with torch.no_grad():\n",
    "        image = vae.decode(latents).sample\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image * 255).round().astype(\"uint8\")\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "    return pil_images\n",
    "\n",
    "def text_enc(prompts, maxlen=None):\n",
    "    '''\n",
    "    A function to take a texual promt and convert it into embeddings\n",
    "    '''\n",
    "    if maxlen is None: maxlen = tokenizer.model_max_length\n",
    "    inp = tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\")\n",
    "    return text_encoder(inp.input_ids)[0]\n",
    "\n",
    "def prompt_2_img(prompts, neg_prompts=None, g=7.5, seed=100, steps=70, dim=512, save_int=False):\n",
    "    \"\"\"\n",
    "    Diffusion process to convert prompt to image\n",
    "    \"\"\"\n",
    "    # input_ids를 float32로 변환\n",
    "    input_ids = text_enc(prompts).to(torch.float32)\n",
    "\n",
    "    # 다음으로 이어지는 부분은 변경 없음\n",
    "    bs = len(prompts)\n",
    "\n",
    "    # Converting textual prompts to embedding\n",
    "    text = text_enc(prompts)\n",
    "\n",
    "    # Adding negative prompt condition\n",
    "    if not neg_prompts:\n",
    "        uncond = text_enc([\"\"] * bs, text.shape[1])\n",
    "    else:\n",
    "        uncond = text_enc(neg_prompts, text.shape[1])\n",
    "    emb = torch.cat([uncond, text])\n",
    "\n",
    "    # Setting the seed\n",
    "    if seed: torch.manual_seed(seed)\n",
    "\n",
    "    # Initiating random noise\n",
    "    latents = torch.randn((bs, unet.in_channels, dim//8, dim//8), dtype=torch.float32)  # dtype을 명시적으로 float32로 설정\n",
    "\n",
    "    # Setting number of steps in scheduler\n",
    "    scheduler.set_timesteps(steps)\n",
    "\n",
    "    # Adding noise to the latents\n",
    "    latents = latents * scheduler.init_noise_sigma\n",
    "\n",
    "    # Iterating through defined steps\n",
    "    for i, ts in enumerate(tqdm(scheduler.timesteps)):\n",
    "        # We need to scale the input latents to match the variance\n",
    "        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n",
    "\n",
    "        # Predicting noise residual using U-Net\n",
    "        with torch.no_grad():\n",
    "            u, t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n",
    "\n",
    "        # Performing Guidance\n",
    "        pred = u + g * (t - u)\n",
    "\n",
    "        # Conditioning the latents\n",
    "        latents = scheduler.step(pred, ts, latents).prev_sample\n",
    "\n",
    "        # Saving intermediate images\n",
    "        if save_int:\n",
    "            if not os.path.exists(f'./steps'): os.mkdir(f'./steps')\n",
    "            latents_to_pil(latents)[0].save(f'steps/{i:04}.jpeg')\n",
    "\n",
    "    # Returning the latent representation to output an image of 3x512x512\n",
    "    return latents_to_pil(latents)\n",
    "\n",
    "# 이미지 생성 및 표시\n",
    "images = prompt_2_img(prompts=prompts, neg_prompts=[\"body, hair, face, eyes, nose, ears, lip, legs, arms, hands, feet, girl, boy, people\"], steps=50, save_int=False)[0]\n",
    "\n",
    "# 수정된 이미지 표시\n",
    "plt.imshow(images)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "    # 1. gpu대신 cpu를 사용할 수 있도록 변경\n",
    "    # 2. \"LayerNormKernelImpl\" not implemented for 'Half' 오류-> 모델을 cpu로 이동하여 실행\n",
    "    # 3. 그래도 계속 같은 오류 발생 -> 모델 수정(모델을 float32 데이터 타입으로 변경)-> text_encoder 모델을 'Float' 데이터 타입으로 변경\n",
    "    # 4. value cannot be converted to type at::Half without overflow 오류 발생->해당 부분을 강제로 'Float' 데이터 타입으로 변환\n",
    "    # 5. local variable 'inp' referenced before assignment 오류->inp변수 제거\n",
    "    # 6. value cannot be converted to type at::Half without overflow 오류 발생->명시적으로 float32->같은 오류\n",
    "    # 7.  모델과 입력 데이터가 모두 float32로 변환->또 같은 오류->모델 내부에서 half 로 처리하는 부분이 남아있을 것으로 보임\n",
    "    # -> 모델을 다시 학습 & 모델 내부에 접근해 수정 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 이미지 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장할 폴더 경로 설정\n",
    "output_folder = \"result\"\n",
    "\n",
    "# 폴더가 존재하지 않으면 생성\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# 생성된 이미지 파일의 이름\n",
    "output_file_name = f\"{new_title}.jpg\"\n",
    "\n",
    "# 이미지 파일의 전체 경로 설정\n",
    "output_file_path = os.path.join(output_folder, output_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 저장\n",
    "images.save(output_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
