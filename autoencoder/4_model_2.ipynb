{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import distutils.dir_util\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoder Basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json write & load 함수 정의\n",
    "def write_json(data, fname):\n",
    "    def _conv(o):\n",
    "        if isinstance(o, (np.int64, np.int32)):\n",
    "            return int(o)\n",
    "        raise TypeError\n",
    "\n",
    "    parent = os.path.dirname(fname)\n",
    "    distutils.dir_util.mkpath(parent)\n",
    "    with io.open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json_str = json.dumps(data, ensure_ascii=False, default=_conv)\n",
    "        f.write(json_str)\n",
    "        \n",
    "def load_json(fname):\n",
    "    with open(fname, encoding='utf-8') as f:\n",
    "        json_obj = json.load(f)\n",
    "\n",
    "    return json_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45824, 24666)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/train_onehot.pkl', 'rb') as f:\n",
    "    train_onehot = pickle.load(f)\n",
    "\n",
    "train_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11456, 24666)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/test_onehot.pkl', 'rb') as f:\n",
    "    test_onehot = pickle.load(f)\n",
    "\n",
    "test_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24666"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/mfl_col.pkl', 'rb') as f:\n",
    "    mfl_col = pickle.load(f)\n",
    "\n",
    "len(mfl_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_len = 22798\n",
    "# song = 22798, tag = 1868"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11456 entries, 0 to 11455\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   tags          11456 non-null  object\n",
      " 1   id            11456 non-null  int64 \n",
      " 2   plylst_title  11456 non-null  object\n",
      " 3   songs         11456 non-null  object\n",
      " 4   like_cnt      11456 non-null  int64 \n",
      " 5   updt_date     11456 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 537.1+ KB\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_json('data/test.json')\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import save_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> model_1 완료\n",
    "- input_dim = 24666 #len(mfl_col)\n",
    "- hidden_dim = 64\n",
    "- dropout = 0.2\n",
    "- BatchNormalization(input_shape=(450,))\n",
    "- LeakyReLU(alpha=0.01)\n",
    "- epoch = 10\n",
    "- batch_size = 64\n",
    "- loss_function = binary_crossentropy\n",
    "- learning_rate = 0.0005\n",
    "- optimizer = adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> model_2\n",
    "- input_dim = 24666 #len(mfl_col)\n",
    "- hidden_dim = 64\n",
    "- dropout = 0.2\n",
    "- BatchNormalization(input_shape=(450,))\n",
    "- LeakyReLU(alpha=0.01)\n",
    "- epoch = 10\n",
    "- batch_size = 64\n",
    "- loss_function = binary_crossentropy\n",
    "- learning_rate = 0.0005\n",
    "- optimizer = adam(learning_rate=learning_rate)\n",
    "- 레이어 하나 더 추가 (32) # 성재님꺼랑 합치기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> model_3\n",
    "- model_2 +\n",
    "- 추가한 레이어에 활성화 함수 leack relu 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_3\n",
    "# input_dim = len(mfl_col)\n",
    "# hidden_dim = 64\n",
    "# dropout_rate = 0.2\n",
    "\n",
    "# inputs = Input(shape=(input_dim,))\n",
    "# encoded = Dropout(0.2)(inputs)\n",
    "# encoded = Dense(hidden_dim)(encoded)\n",
    "# encoded = BatchNormalization(input_shape=(hidden_dim,))(encoded)\n",
    "# encoded = LeakyReLU(alpha=0.01)(encoded)\n",
    "# encoded = Dense(36)(encoded) # 36개로 축소(인코딩)\n",
    "# encoded = LeakyReLU(alpha=0.01)(encoded)\n",
    "\n",
    "# decoded = Dense(input_dim,activation='sigmoid')(encoded)\n",
    "# model_3 = Model(inputs, decoded, name='autoencoder')\n",
    "# model_3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> model_4\n",
    "- model_3 +\n",
    "- 추가한 레이어에 배치정규화도 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_4\n",
    "# input_dim = len(mfl_col)\n",
    "# hidden_dim = 64\n",
    "# dropout_rate = 0.2\n",
    "\n",
    "# inputs = Input(shape=(input_dim,))\n",
    "# encoded = Dropout(0.2)(inputs)\n",
    "# encoded = Dense(hidden_dim)(encoded)\n",
    "# encoded = BatchNormalization(input_shape=(hidden_dim,))(encoded)\n",
    "# encoded = LeakyReLU(alpha=0.01)(encoded)\n",
    "# encoded = Dense(36)(encoded) # 36개로 축소(인코딩)\n",
    "# encoded = BatchNormalization(input_shape=(hidden_dim,))(encoded)\n",
    "# encoded = LeakyReLU(alpha=0.01)(encoded)\n",
    "\n",
    "# decoded = Dense(input_dim,activation='sigmoid')(encoded)\n",
    "# model_4 = Model(inputs, decoded, name='autoencoder')\n",
    "# model_4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> model_5\n",
    "- input_dim = 24666 #len(mfl_col)\n",
    "- hidden_dim = 450\n",
    "- dropout = 0.2\n",
    "- BatchNormalization(input_shape=(450,))\n",
    "- LeakyReLU(alpha=0.01)\n",
    "- epoch = 41\n",
    "- batch_size = 256\n",
    "- loss_function = binary_crossentropy\n",
    "- learning_rate = 0.0005\n",
    "- optimizer = adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> model_6\n",
    "- input_dim = 24666 #len(mfl_col)\n",
    "- hidden_dim = 450\n",
    "- dropout = 0.2\n",
    "- BatchNormalization(input_shape=(450,), trainable=False)\n",
    "- LeakyReLU(alpha=0.01)\n",
    "- epoch = 41\n",
    "- batch_size = 256\n",
    "- loss_function = binary_crossentropy\n",
    "- learning_rate = 0.0005\n",
    "- optimizer = adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 24666)]           0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24666)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                1578688   \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 64)               256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 36)                2340      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 24666)             912642    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,493,926\n",
      "Trainable params: 2,493,798\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model_2\n",
    "input_dim = len(mfl_col)\n",
    "hidden_dim = 64\n",
    "dropout_rate = 0.2\n",
    "\n",
    "inputs = Input(shape=(input_dim,))\n",
    "encoded = Dropout(0.2)(inputs)\n",
    "encoded = Dense(hidden_dim)(encoded) # 64개로 축소(인코딩)\n",
    "encoded = BatchNormalization(input_shape=(hidden_dim,))(encoded)\n",
    "encoded = LeakyReLU(alpha=0.01)(encoded)\n",
    "encoded = Dense(36,activation='relu')(encoded) # 36개로 축소(인코딩)\n",
    "\n",
    "decoded = Dense(input_dim,activation='sigmoid')(encoded)\n",
    "model_2 = Model(inputs, decoded, name='autoencoder')\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0005\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "model_2.compile(optimizer=optimizer,loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. 셀의 코드를 검토하여 오류의 가능한 원인을 식별하세요. 자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'> 여기 </a> 를 클릭하세요. 자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "model_2.fit(train_onehot,train_onehot,epochs=10, batch_size=64, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.save('results/model_2_0929.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = tf.keras.models.load_model('results/model_2_0929.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_plist=model_2.predict(test_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_id = list(test['id'])\n",
    "col= mfl_col\n",
    "ori_song = col[:song_len]\n",
    "ori_tag = col[song_len:]\n",
    "\n",
    "song_predict = predict_plist[:,:song_len] # song output(추천곡)\n",
    "tag_predict = predict_plist[:,song_len:] # tag output(추천태그)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=[]\n",
    "n=0\n",
    "for i in df_id:\n",
    "    dic={}\n",
    "    dic['id']=i\n",
    "\n",
    "    plist_song=song_predict[n].argsort()[-100:] # predict한 song output 중 상위 100개\n",
    "    p_song=[]\n",
    "    for song in plist_song:\n",
    "        p_song.append(ori_song[song])\n",
    "    dic['songs']=p_song\n",
    "\n",
    "    plist_tag=tag_predict[n].argsort()[-10:] # predict한 tag output 중 상위 10개\n",
    "    p_tag=[]\n",
    "    for tag in plist_tag:\n",
    "        p_tag.append(ori_tag[tag])\n",
    "    dic['tags']=p_tag\n",
    "    n+=1\n",
    "    result.append(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json(result,'results/result_model_2.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArenaEvaluator:\n",
    "    def _idcg(self, l):\n",
    "        return sum((1.0 / np.log(i + 2) for i in range(l)))\n",
    "\n",
    "    def __init__(self):\n",
    "        self._idcgs = [self._idcg(i) for i in range(101)]\n",
    "\n",
    "    def _ndcg(self, gt, rec):\n",
    "        dcg = 0.0\n",
    "        for i, r in enumerate(rec):\n",
    "            if r in gt:\n",
    "                dcg += 1.0 / np.log(i + 2)\n",
    "        if len(gt)>100:\n",
    "            gt = gt[:100]\n",
    "        return dcg / self._idcgs[len(gt)]\n",
    "\n",
    "    def _eval(self, gt_fname, rec_fname):\n",
    "        gt_playlists = load_json(gt_fname)\n",
    "        gt_dict = {g[\"id\"]: g for g in gt_playlists}\n",
    "        rec_playlists = load_json(rec_fname)\n",
    "        gt_ids = set([g[\"id\"] for g in gt_playlists])\n",
    "        rec_ids = set([r[\"id\"] for r in rec_playlists])\n",
    "        if gt_ids != rec_ids:\n",
    "            raise Exception(\"결과의 플레이리스트 수가 올바르지 않습니다.\")\n",
    "\n",
    "        rec_song_counts = [len(p[\"songs\"]) for p in rec_playlists]\n",
    "        rec_tag_counts = [len(p[\"tags\"]) for p in rec_playlists]\n",
    "        if set(rec_song_counts) != set([100]):\n",
    "            raise Exception(\"추천 곡 결과의 개수가 맞지 않습니다.\")\n",
    "\n",
    "        if set(rec_tag_counts) != set([10]):\n",
    "            raise Exception(\"추천 태그 결과의 개수가 맞지 않습니다.\")\n",
    "\n",
    "        rec_unique_song_counts = [len(set(p[\"songs\"])) for p in rec_playlists]\n",
    "        rec_unique_tag_counts = [len(set(p[\"tags\"])) for p in rec_playlists]\n",
    "\n",
    "        if set(rec_unique_song_counts) != set([100]):\n",
    "            raise Exception(\"한 플레이리스트에 중복된 곡 추천은 허용되지 않습니다.\")\n",
    "\n",
    "        if set(rec_unique_tag_counts) != set([10]):\n",
    "            raise Exception(\"한 플레이리스트에 중복된 태그 추천은 허용되지 않습니다.\")\n",
    "\n",
    "        music_ndcg = 0.0\n",
    "        tag_ndcg = 0.0\n",
    "\n",
    "        for rec in rec_playlists:\n",
    "            gt = gt_dict[rec[\"id\"]]\n",
    "            music_ndcg += self._ndcg(gt[\"songs\"], rec[\"songs\"][:100])\n",
    "            tag_ndcg += self._ndcg(gt[\"tags\"], rec[\"tags\"][:10])\n",
    "\n",
    "        music_ndcg = music_ndcg / len(rec_playlists)\n",
    "        tag_ndcg = tag_ndcg / len(rec_playlists)\n",
    "        score = music_ndcg * 0.85 + tag_ndcg * 0.15\n",
    "\n",
    "        return music_ndcg, tag_ndcg, score\n",
    "\n",
    "    def evaluate_with_save(self, gt_fname, rec_fname, model_file_path, default_file_path):\n",
    "        # try:\n",
    "        music_ndcg, tag_ndcg, score = self._eval(gt_fname, rec_fname)\n",
    "        with open(f'{default_file_path}/results.txt','a') as f:\n",
    "            f.write(model_file_path)\n",
    "            f.write(f\"\\nMusic nDCG: {music_ndcg:.6}\\n\")\n",
    "            f.write(f\"Tag nDCG: {tag_ndcg:.6}\\n\")\n",
    "            f.write(f\"Score: {score:.6}\\n\\n\")\n",
    "            print(f\"Music nDCG: {music_ndcg:.6}\")\n",
    "            print(f\"Tag nDCG: {tag_ndcg:.6}\")\n",
    "            print(f\"Score: {score:.6}\")\n",
    "        # except Exception as e:\n",
    "        #     print(e)\n",
    "\n",
    "    def evaluate(self, gt_fname, rec_fname):\n",
    "        # try:\n",
    "        music_ndcg, tag_ndcg, score = self._eval(gt_fname, rec_fname)\n",
    "        print(f\"Music nDCG: {music_ndcg:.6}\")\n",
    "        print(f\"Tag nDCG: {tag_ndcg:.6}\")\n",
    "        print(f\"Score: {score:.6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_fname = 'data/test.json'\n",
    "rec_fname = 'results/result_model_2.json'\n",
    "arena_evaluator = ArenaEvaluator\n",
    "arena_evaluator.evaluate(gt_fname, rec_fname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
